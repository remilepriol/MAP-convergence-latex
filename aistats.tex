\documentclass[twoside]{article}

\usepackage{silence}
% Disable all warnings issued by latex starting with "You have..."
% This is a hack, there is no clean solution for that unless we install locally the package
\WarningFilter{latex}{You have requested package}




\pdfsuppresswarningpagegroup=1 %removes the warning
% This is a hack, the other way to remove is to convert all pdf into postscript or eps.


\usepackage{aistats/aistats2022}
% If your paper is accepted, change the options for the package
% aistats2022 as follows:
%\usepackage[accepted]{aistats2022}

\setlength{\footskip}{3.31pt}

% If you set papersize explicitly, activate the following three lines:
%\special{papersize = 8.5in, 11in}
%\setlength{\pdfpageheight}{11in}
%\setlength{\pdfpagewidth}{8.5in}

\let\oldsection\section
\renewcommand{\section}[1]{\oldsection{\texorpdfstring{\uppercase{#1}}{#1}}}

% If you use natbib package, activate the following three lines:
\usepackage[round]{natbib}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{lmodern}
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\graphicspath{{figs/}}
%\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{placeins}
\usepackage{hyperref}       % hyperlinks
\usepackage[dvipsnames]{xcolor}

\usepackage{array}

\hypersetup{ % SLJ: my standard paper setup...
	pdftitle={MAP convergence},
	pdfkeywords={},
	pdfborder=0 0 0,
	pdfpagemode=UseNone,
	colorlinks=true,
	linkcolor=blue, %mydarkblue,
	citecolor=blue, %mydarkblue,
	filecolor=blue, %mydarkblue,
	urlcolor=blue, %mydarkblue,
	pdfview=FitH,
	pdfauthor={Anonymous},
	% draft,
	final,
}

\usepackage[capitalise]{cleveref}
\newcommand{\rlp}[1]{\textcolor{BrickRed}{(RLP:#1)}}
\newcommand{\fdk}[1]{\textcolor{Periwinkle}{(fdk:#1)}}
\newcommand{\TODO}[1]{\textcolor{cyan}{(TODO #1)}}
\newcommand{\tocite}{\textcolor{purple}{(add citation)}}

% my packages
\usepackage{math_commands}
% some custom math commands
\newtheorem{proposition}{Proposition}
\newtheorem{problem}{Open Problem}

\newcommand*{\expect}[2][]{\ensuremath{\mathbb{E}_{#1} \left[ #2 \right] }} % expectation operator
\newcommand*{\expecti}[2][]{\ensuremath{\mathbb{E}_{#1} [ #2 ] }} % expectation operator

\newcommand{\cond}{\,\vert\,}
\newcommand{\logpart}{A}
\newcommand{\conj}{\logpart^*}
\newcommand{\bregman}{\cB_\logpart}
\newcommand{\bregmanconj}{\cB_{\logpart^*}}
\newcommand{\nat}{\theta}
\newcommand{\m}{\mu}
\newcommand{\meanp}{\m}
\newcommand{\decrement}{D}
\newcommand{\linear}{\ell} % linearization of a function
\newcommand{\lr}{\gamma} % learning rate, or step-size
\newcommand{\lin}[1]{\left\langle#1\right\rangle}

\newcommand{\MAPm}{\hat \m_n}
\newcommand{\MAPt}{\hat \nat_n}
\DeclareMathSymbol{\shortminus}{\mathbin}{AMSa}{"39}

\newcommand{\stgcvx}{\alpha} % strong convexity
\newcommand{\smooth}{\beta} % smoothness

\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{Convergence Rates for the MAP of an Exponential Family\\ and Stochastic Mirror Descent -- an Open Problem}


\aistatsauthor{R\'emi Le Priol \And Frederik Kunstner \And  Damien Scieur \And Simon Lacoste-Julien }

\aistatsaddress{ Mila \And  UBC \And SAIT SAIL \And Mila}
]

\begin{abstract}
We consider the problem of upper bounding the expected log-likelihood sub-optimality of the maximum likelihood estimate (MLE), or a conjugate maximum a posteriori (MAP) for an exponential family, in a non-asymptotic way.
Surprisingly, we found no general solution to this problem in the literature. In particular, current theories do not hold for a Gaussian or in the interesting few samples regime.
After exhibiting various facets of the problem, we show we can interpret the MAP as running stochastic mirror descent (SMD) on the log-likelihood. However, modern convergence results do not apply for standard examples of the exponential family, highlighting holes in the convergence literature.
We believe solving this very fundamental problem may bring progress to both the statistics and optimization communities.
\end{abstract}

%In particular, no rates hold in the few samples regime  -- e.g. after seeing 5 samples from a gaussian, we do not know how many bits away from the true distribution we should expect our model to be.
%No rates hold either when the loss is ill-behaved : infinite domain, not smooth, not self-concordant


\begin{figure}[t]
	\centering
\includegraphics[width=.4\textwidth]{fewsamples.pdf}
	\caption{KL divergence~\eqref{eq:suboptimalityKL} for Gaussian variance MLE (\S\ref{ssec:gaussian-variance}) against number of samples $n$. The bold curve plots the average over 10000 trials,  dark shaded area is 90\% confidence interval, and light shade is a min-max interval.
		The expected value is infinite for $n=1$ and $n=2$, but for $n\geq3$ it quickly joins the upper bound~\eqref{eq:MLE_rate} and the $1/2n$ asymptote~\eqref{eq:asymptote}, while the variance remains large.
		We wish to find similar upper bounds for a variety of exponential families.
	}
	\label{fig:curves}
\end{figure}


\section{Introduction}
\label{sec:motivation}

{\bf Exponential families} are among the most widely used simple parametric models of data, yet, we will highlight some open problems about them in this paper.
Many standard random variables are exponential families: Gaussians, categorical, gamma, or Dirichlet, for example.
They are flexible enough to model a variety of data sources $X$ and easy to describe with some sufficient statistics $T(X) \in \real^d$.
They are particularly appreciated for their convex log-likelihood
\alignn{
f(\nat) := \E[-\log p_\nat(X)] = \logpart(\nat) - \lin{\E[T(X)] , \nat},
\label{eq:defNLL}
}
where $\logpart$ is the convex log-partition function and $\nat\in\Theta$ is the \textit{natural parameter}.
This convexity lays the foundation for generalized linear models \citep{mccullagh1989generalized} or variants of principal component analysis \citep{collins2001generalization}, among other applications.

{\bf Estimators.}
In this paper, we consider the problem of estimating $\nat$ from a dataset $\mD = (X_1, \dots, X_n)$ of iid observations from $p_\theta$ in an exponential family.
In this case, not only is $f$ convex, but it yields a simple condition for the maximum-likelihood estimate (MLE)
\begin{align}
 \hat \mu_n^\text{MLE} = \nabla  \logpart(\hat \nat_n^\text{MLE}) = \frac{\sum_{i=1}^n T(x_i)}{n} \; .
	\label{eq:defMLE}
\end{align}
This rule is also known as moment matching.
Given a specific conjugate prior, a similar formula~\eqref{eq:defMAP} holds for the maximum a posteriori (MAP). In this paper, we will focus on analyzing MLE and MAP estimators.\footnote{A related analysis is present in the online-learning literature, but for different online estimators, which are less efficient than offline methods \citep{azoury2001relative,dasgupta2007online}.}

{\bf Statistical decision theory.}
To assess the quality of an estimator $\hat \nat$ (and compare them), we need to define some notion of closeness to the correct parameter $\nat^*$.
We distinguish here two ways: \textit{distance in parameter space} and \textit{``distance'' between distributions}.
{\bf 1)} Distance in parameter space $d(\nat,\nat^*)$. This is the focus of \emph{parameter estimation}, yielding results such as the asymptotic efficiency of the MLE via the Cramer-Rao lower-bound \citep{aitken1942estimation} and a wealth of asymptotic results \citep{vdv1998asymptotic}.
In particular for sum of independent variables such as~\eqref{eq:defMLE}, large deviations theory \citep{varadhan1984large} characterizes concentration phenomena.
{\bf 2)} Distance between distributions, as studied in \emph{density estimation}.
For this purpose, the Kullback-Leibler (KL) divergence $\KL\paren{p_{\nat^*} || p_{\nat} }$  arises naturally from information theory,
but its lack of robustness to misspecification\footnote{
For $p$ and $q$ continuous densities,
$\KL(p||q) = +\infty$ if $\exists x, q(x)=0 \, \& \, p(x)>0$.
%Since exponential families have a positive density over their whole input set, this is not a concern for us, except if we misspecified the input set.
}
has led statisticians to study symmetric, better-behaved distances, such as the $L^2$ norm \citep[\S1.2]{tsybakov2009introduction}, the $L^1$ norm \citep{devroye2001combinatorial} or more recently the Hellinger distance \citep{baraud2017new}.
With exponential families, the KL divergence is also a Bregman divergence between parameters (see \S\ref{sec:problem}), thus drawing a connection between these two lines of research.
We thus consider the fundamental problem:
%\begin{center}
%	\framebox{
%		\parbox[c][]{0.9\linewidth}{
%			\begin{center}
%			\emph{Can we upper bound  the expected value of the KL divergence for MLE or MAP?}
%		\end{center}
%		}
%	}
%\end{center}
\begin{equation}
\boxed{
\begin{aligned}
	\textit{Find an upper}&\textit{ bound on the expected value of } \\
	&\KL(p_{\nat^*} || p_{\hat \nat_n^\text{MLE/MAP} }) \; .
\end{aligned}
}
\tag{$\star$}
\label{problem}
\end{equation}
% RLP : that is a dirty hack, but I would like to be able to reference the problem somehow.

There are already general asymptotic results (\S\ref{ssec:asymptote}), and a finite $n$ result when $\logpart$ is quadratic (e.g., $X$ is Gaussian with known variance)
or close to quadratic (\S\ref{ssec:quadratic}).
However, a  general solution for finite $n$ remains elusive.
In this paper, we review partial solutions and give ideas on how to solve the problem.

{\bf Stochastic optimization} offers an interesting perspective on this question.
Consider the problem
\alignn{
	\min_{\theta\in \Theta} f(\theta)\,,
	\label{eq:optimization_problem}
}
solved by $\nat^*\in \Theta$.
Setting $f$ as the log-likelihood~\eqref{eq:defNLL}, the suboptimality is equal to the KL:
\alignn{
	f(\nat) - f(\nat^*) = \KL\paren{p_{\nat^*} || p_{\nat} } .
	\label{eq:suboptimalityKL}
}
Both MLE and MAP can be seen as stochastic algorithms solving~\eqref{eq:optimization_problem}.
In particular, with exponential families, MAP is equivalent to stochastic mirror descent (SMD) \citep{nemirovski2009robust}.
Inspired by recent work~\citep{lepriol2021analysis, kunstner2020homeomorphic}, we consider using existing convergence rates for SMD to get the upper bound we seek.
Unfortunately, none of the current analyses apply, highlighting open problems for the analysis of SMD.

{\bf Expected Outcomes.}
A solution to~\eqref{problem} can clarify the importance of the prior in MAP, in particular in the few sample regime. %, for instance for Gaussians $\cN(m,\sigma^2)$.
Also, it could enable stochastic optimization to tackle a broad class of barrier objectives.\footnote{we call \emph{barrier} an objective $f$ that is infinite on the boundaries of its domain (assuming they exist).}
A good example is the generalized linear model based on Gaussians with unknown mean and variance, for which there is currently no theory \citep{bach2013nonstronglyconvex}.
It could also help assess the impact of alternative forms of regularization (prior) for these models.

{\bf Contributions.}
After formalizing the problem~\eqref{problem} (\S\ref{sec:problem}), along with its asymptotic properties (\S\ref{ssec:asymptote}), we make the following contributions.
\begin{itemize}
	\itemsep0em
	\item We provide an upper bound on the KL in the particular case of a Gaussian with known mean but unknown variance $\cN(0,\sigma^2)$ (\S\ref{ssec:gaussian-variance}), illustrating that tight rates are possible even though the current theory does not cover them.
	\item We highlight sufficient conditions to characterize when a (local) quadratic approximation of the KL is valid, offering a partial answer to~\eqref{problem} (\S\ref{ssec:quadratic}-\ref{ssec:local-quadratic}).
	%SLJ REMOVED per Slack discussion
	%\item We leverage a generalized bias-variance decomposition of Bregman divergences\citep{pfau2013generalized}  to formulate a conjecture about~\eqref{problem}(\S\ref{ssec:bias-variance}).
	\item By linking MAP and SMD, we show that modern analysis of SMD is yet to prove convergence on barrier objectives such as $-\log$ (\S\ref{sec:optimization}).
\end{itemize}


{\bf Notation.}
$X$ and $T=T(X)$ are random variables, $x$ is a sample, $n$ is the number of samples and $d= \dim(T)$.
$\langle \cdot , \cdot \rangle$ is the Euclidean scalar product in $\real^d$.


\section{Technical background}
\label{sec:background}
This section reviews the formalism of exponential families, their duality, a conjugate prior, and the corresponding MAP.
We point the reader towards \citet[Chapter 3]{wainwright2008graphical} for a more detailed introduction.


The density of an exponential family for a sample $x$ is
\begin{equation}
	 p_\nat(x) = p(x|\nat) = \exp( \langle \nat, T(x) \rangle - \logpart(\nat)) \; ,
	 \label{eq:def_expfamily}
\end{equation}
where  $\nat$ is called natural (or primal) parameter.
It is fully specified by 1) $T: \cX \rightarrow \real^d$, the sufficient statistic,
and 2) a base measure $\nu$ on $\cX$.
Since the exponential is positive, $p$ has the same support as $\nu$.
The log-partition function $\logpart$ acts as a normalization term, since
\begin{align}
    \logpart(\nat) = \log \int e^{\langle \nat, T(x) \rangle} \nu(dx) \;.
\end{align}
This simple model encompasses both categorical distributions : $\cX = \{1, \dots, k\}$, $\nu$ uniform and $T(X)$  the one-hot encoding and multivariate normal distributions $\cX=\real^d, \nu$ Lebesgue and $T(X)=(X, X X^\top)$.

For convenience, we focus on steep, regular exponential families with minimal statistic $T$ \citep{barndoffnielsen2014information}.
Then $\logpart$ is a strictly convex function of Legendre type,
and the set $\Theta = \{ \nat \cond \logpart(\nat) < \infty\}$ is open and convex.
When explicit, we write the random variable $T = T(X)$.

% TODO sort out relationships between : steep family, minimal statistics and Legendre type. What implies what ?

{\bf Duality.}
The log-partition function $\logpart$ verifies the two following identities:
\begin{align}
    \nabla\logpart(\nat) &=  \expect[p_\nat]{T(X)} =: \meanp, \\
    \nabla^2 \logpart(\nat) &= \Cov_{p_\nat}[T(X)] > 0,
\end{align}
where $\meanp$ is called the mean (or dual) parameter, which lives in the open convex set $\cM$ equal to the relative interior of the convex hull of $T(\cX)$.
Given that $\logpart$ is strictly convex, its Hessian is positive definite, and its gradient $\nabla \logpart$ is a \textit{bijection} between natural parameters $\nat$ and mean parameters $\m$.
We will write $\m$ or $\nat$ interchangeably depending on the context, being aware that both are linked and represent the same distribution.

We now introduce the convex conjugate (the Fenchel-Legendre transform) of the log-partition function
\begin{align}
	\conj(\m) =  \langle \m, \nat \rangle - \logpart(\nat)
	=  \max_{\nat'\in\Theta}  \langle \m, \nat' \rangle - \logpart(\nat')\; ,
\end{align}
which is the common notion of \textit{entropy} in information theory.
By Fenchel duality, its gradient is the inverse of the gradient of $\logpart$,  $\nabla\conj=\nabla\logpart^{-1}$, giving
\aligns{
	\nabla\conj \circ \nabla\logpart(\nat) = \nat, \quad \nabla\logpart\circ \nabla\conj(\meanp) = \meanp.
}
%\begin{figure}[t]
%	\centering
%	\includegraphics{duality}
%	\caption{The gradient of the log-partition function and its dual, $(\nabla \logpart, \nabla \conj)$, form a bijection between natural and mean parameters $\nat, \meanp$. Figure copied from \citet{kunstner2020homeomorphic}. % TODO make an original one.
%	}
%	\label{fig:duality}
%\end{figure}%
%
%Not super convinced about introducing that here? it comes naturally when considering the KL on exponential maps) %RLP it is useful to the conjugate prior.
{\bf The Bregman Divergence} induced by $\logpart$ measures the discrepancy between two parameters $\nat$ and $\nat_0$,
\begin{align}
    \bregman (\nat ; \nat_0)
    & = \logpart(\nat) - \logpart(\nat_0)
    - \langle \nabla \logpart(\nat_0)  , \nat - \nat_0 \rangle,
    \label{eq:defBregman}
\end{align}
with $\nabla \logpart(\nat_0) = \expect[\nat_0]{T(X)} =: \meanp_0$ the mean parameter associated to $\nat_0$.
In general, Bregman divergences are not symmetric, i.e., $\bregman (\nat ; \nat_0)\neq \bregman (\nat_0 ; \nat)$.
%Finally, it is equal to the divergence of its convex conjugate with switched arguments,
%\begin{align}
%	\bregman (\nat ; \nat_0)
%    = \bregmanconj ( \meanp_0 ; \meanp) \; .
%    \label{eq:bregman_switch}
%\end{align}

{\bf A Conjugate Prior} for $p(X|\nat)$ is
\begin{align}
    p(\nat)
    &\propto \exp( - n_0 \bregman(\nat ; \nat_0) ) \\
    &\propto \exp(n_0 \langle \m_0, \nat \rangle - n_0 \logpart(\nat)),
    \label{eq:def_prior}
\end{align}
where $n_0$ and $\nat_0$ are (hyper)parameters of the prior  \citep{agarwal2010geometric}.
This is an exponential family with sufficient statistics $(\nat ,\logpart(\nat))$ and natural parameter $(n_0 \m_0, -n_0)$.
Intuitively, $n_0$ is the number of fictive data points observed from a distribution with natural parameter $\nat_0$.

{\bf Maximum A Posteriori (MAP).}
Given a dataset $\mD_n =(X_1,\dots,X_n)$, we wish to estimate the maximum of the posterior distribution $p(\nat \cond \mD_n) \propto p(\mD_n|\nat)p(\nat)$.
Plugging in~\eqref{eq:def_expfamily},~\eqref{eq:defBregman} and~\eqref{eq:def_prior} yields
\begin{align}
	p(\nat \cond \mD_n)
    \propto \exp(- (n_0+n) \bregman(\nat; \MAPt^\text{MAP}))
    \label{eq:joint_likelihood}
\end{align}
which reaches its maximum at $\MAPt^\text{MAP}$ such that
\begin{align}
    \nabla \logpart(\MAPt^\text{MAP}) = \MAPm^\text{MAP}
    = \frac{n_0 \meanp_0 + \sum_{i=1}^n T_i}{n_0+n} \; ,
    \label{eq:defMAP}
\end{align}
where $T_i=T(X_i)$.
When $n_0=0$ (no samples from the prior), we recover the MLE~\eqref{eq:defMLE}.
We write $\MAPt$ for the MAP and view the MLE as a particular case.


\section{PROBLEMS FORMULATION}
\label{sec:problem}
% Assume we observe a dataset $\mD$ drawn i.i.d. from a distribution $\cD$.
% We further assume that our model is well-specified and there exist $\nat^*\in\Theta$ such that $\cD = p(\cdot \cond \nat)$.
% As we will see, this is already an interesting setting.
% We wish to estimate how well the MLE or a MAP model the true distribution.

We are now ready to formalize the main problem of this paper. Assume we observe a dataset $\mD_n$ drawn i.i.d. from $p(\cdot \cond\nat^*)$, an exponential family distribution
with parameters $\nat^*$.
We wish to quantify how well the MLE or a MAP approximates the true distribution.

A natural way to quantify this is the Kullback-Leibler divergence (KL) $\KL(p_{\nat^*} || p_\nat)$. % should we define it ?
In the well-specified setting, it corresponds to the log-likelihood sub-optimality~\eqref{eq:suboptimalityKL}.
With exponential families, the KL is also a Bregman divergence:
\alignn{
	\KL(p_{\nat^*} || p_\nat)
	 = \bregman(\nat ; \nat^*)
	 = \bregmanconj(\m^* ; \m) \; .
}
The second equality is a general property of Bregman divergences and convex conjugates. % (Be careful about the order of arguments.)
How does this quantity behave when $\hat \nat$ is the MLE or MAP?
 Or in the words of statistical decision theory, what is the \emph{frequentist risk} of these estimators when the loss is the KL divergence?
This is our first problem.

\begin{problem}[Upper-bounding MAP and MLE]
Upper bound the following quantities:
\begin{align}
	\label{eq:bregmanMLE}
	\text{MLE: } \quad &\expect[\mD_n]{\bregmanconj \left (\E_{\nat^*} [T] ;  \inv{n}  \smallsum_i T_i \right )}, \\
	\label{eq:bregmanMAP}
	\text{MAP: } \quad &\expect[\mD_n]{\bregmanconj \left (\E_{\nat^*} [T] ; \frac{n_0 \m_0 + \smallsum_i T_i}{n_0+n} \right )},
\end{align}
where the expectation is on the data $\mD_n = (T_1, \dots, T_n)$.
\end{problem}

More explicitly, we want an upper bound that does not involve this expectation over the dataset.
Surprisingly, we found no general solution to this seemingly simple problem, whether in the literature or by our means.
In \S\ref{sec:example}, we provide results for special cases such as $\cN(0, \sigma^2)$,
while in \S,\ref{sec:insights} we provide realistic conditions to obtain valid bounds after seeing a large number of samples.
However, we have yet to find a solution encompassing both a broad range of exponential families
and applicable to small sample sizes $n \lesssim d$.

{\bf A Difficulty with the MLE.}
While~\eqref{eq:bregmanMAP} is always finite, \eqref{eq:bregmanMLE} may be infinite,
for instance when estimating the covariance of a Gaussian when $n \leq d + 1$.
Even worse, there is a non-zero probability never to sample one of the categories with categorical variables.
In those cases the MLE gives zero weight to this category and $\KL(p_{\nat^*} \,\Vert\, p_\text{MLE} ) = +\infty$.
Therefore, the expected KL~\eqref{eq:bregmanMLE} is infinite for any number of samples.
Instead of taking the expectation, one might want to bound the risk in high probability
without resorting to Markov inequality, but this is a difficult endeavor.
These examples make a case for regularized estimators such as MAP,
for which we may find upper bounds.

\paragraph{Optimization.}
With exponential families, MAP can be linked to \emph{stochastic mirror descent (SMD)}, see App.~\ref{app:SMD}.
More precisely, let us re-write~\eqref{eq:defMAP} as
\alignn{
\m_n = \m_{n-1}- \lr_n (\m_{n-1} - T_n)
\label{eq:mean_update}
}
where $\lr_n := \inv{n_0 + n}$.
Now define stochastic functions $f_X(\nat) = -\log p(X \cond \nat)$ such that $\E[f_X] = f$.
If we further introduce stochastic gradients $g_n(\nat) := \nabla\logpart(\nat) - T_n = \nabla f_{X_n}(\nat)$, then~\eqref{eq:mean_update} becomes
\alignn{
	\nabla\conj(\hat \nat_{n})
	= \nabla\conj(\hat \nat_{n-1}) - \lr_n g_n(\hat \nat_{n-1}),
}
which is the update formula for SMD on $f$ with mirror map $\nabla\logpart$
and step-size schedule $\lr_n$, initialized at $\nat_0$.
In this view, MLE forgets its (arbitrary) initialization after the first step with step size 1.
The observation MAP $\in$ SMD brings us to our second problem.
\begin{problem}[Convergence rate for SMD]
Find a convergence rate for stochastic mirror descent that applies to conjugate MAP of exponential families such as Gaussians $\cN(\meanp, \sigma^2)$.
\end{problem}

To address these problems, we start by investigating simple examples to provide solutions to Problem 1, getting insights into what is achievable.

\section{Illustrating Examples}\label{sec:example}
\subsection{Gaussian with Unknown Variance}\label{ssec:gaussian-variance}

A non-trivial yet straightforward example is the centered Gaussian distribution with unknown variance $\cN(0,\sigma^2)$.
Its log-likelihood reads $\log p(x) = -\frac{x^2}{2 \sigma^2} - \half\log(2\pi \sigma^2)$.
Defining $T(X)=X^2$ as the sufficient statistic, we get natural parameter $\nat = -\inv{2 \sigma^2} <0$, and mean parameter $\m=\E[T(X)] = \sigma^2 >0$.
Mean and natural parameters are roughly inverse of each other, i.e., $\nat = -\inv{2 \m}$.
Now we match the log-likelihood with the exponential family template to get the log-partition function, and take the conjugate to find the entropy
\aligns{
	\logpart (\nat) = - \half \log(-\nat) \quad\text{and}\quad
	\conj(\m) = - \half \log(\m)  \; ,
}
up to constants.
Both $A$ and  the entropy are roughly negative logarithm $z\mapsto - \log(z)$.
It means the conjugate prior is the exponential family with sufficient statistic $(\nat, \log(-\nat) )$, e.g., a negative gamma distribution.
It also means $\bregman$ and $\bregmanconj$ have the same shape
\begin{align}
	\bregmanconj( \m^*; \m_n)
	&= \half \left ( \frac{\m^*}{ \m_n} - 1 - \log  \frac{\m^*}{ \m_n} \right) \; .
%	\bregman( \nat_n; \nat_* )
%	&=  \half \left ( \frac{ \nat_n}{\nat_*} - 1 - \log  \frac{ \nat_n}{\nat_*} \right) \; .
\end{align}
In Theorems~\ref{thm:varianceMLE} and~\ref{thm:varianceMAP}, we report upper bounds on the expected value of this divergence for the MLE and the MAP.
All proofs for this section are in App.~\ref{app:gaussian-variance}.
\begin{theorem}[MLE Bound]
\label{thm:varianceMLE}
	The MLE of $\cN(0,\m^*)$ is $\hat \m_n^\text{MLE} = \inv{n} \sum_i X_i^2 $.
	Its expected suboptimality is infinite when $n\leq 2$, and otherwise upper-bounded as
	\begin{align}
		 \expect{\bregmanconj( \m^*; \hat \m_n^\text{MLE}) }
			\leq \inv{2n} +\frac{2}{n(n-2)} \; .
			\label{eq:MLE_rate}
	\end{align}
\end{theorem}
This upper bound matches the asymptotic result~\eqref{eq:asymptote} that we derive in \S\ref{ssec:asymptote}.
We illustrate its numerical behavior in Figure~\ref{fig:curves}.
With the same technique, we obtain a similar bound for the multivariate generalization: the expected value is infinite whenever $n \leq d+1$ where $d$ is the dimension, and is otherwise bounded by $O(\frac{d^2}{n} + \frac{d^3}{n(n-d-1)} )$.

\begin{theorem}[MAP Bound]
\label{thm:varianceMAP}
The expected suboptimality of the MAP of $\cN(0,\m^*)$ with prior hyper-parameters $(n_0,\m_0)$ is
 \begin{align}
	& \expect{\bregmanconj( \m^*; \hat \m_n^\mathrm{MAP})}
	\leq \begin{cases}
		\inv{2(n_0+1)}  +  b_1 \ \text{if}\ n=1,\\
		\frac{1}{n_0 \frac{\m_0}{\m^*} +n-2} + b_n \ \text{if}\ n\geq 2
	\end{cases}
	\label{eq:MAP_rate}\\
	& \text{where }b_n = \frac{(1 + \inv{n_0} - \frac{\m_0}{\m^*})^2}{2 (\frac{\m_0}{\m^*}+\frac{\max(0,n-2)}{n_0})(1 + \frac{n}{n_0} )} \; . \nonumber
\end{align}
\end{theorem}
This inequality was derived with the symmetrized Bregman $\cB(a,b) + \cB(b,a)$ for which calculus was more tractable.
This explains the loss of a factor 2 compared to the asymptote~\eqref{eq:asymptote}.
Anticipating on \S\ref{ssec:bias-variance}, this inequality highlights an explicit variance-bias decomposition.
In particular, there is no bias term when $\frac{\m_0}{\m^*} =1 + \inv{n_0} $, which happens when the prior is slightly larger than the ground truth.
This correlates well with our numerical observations (cf App.~\ref{app:gaussian-variance}).

\begin{figure*}[t]
	\centering
	\includegraphics[width=.8\textwidth]{figs/thales/numerical_schema_n=3.pdf}
	\caption{
	Primal and dual representations of a Gaussian $\cN(m,\sigma^2)$ MAP (blue) and MLE (orange) (\S\ref{ssec:gaussian} with $n=3$).
	In dual space, MAP is a scaled version of the MLE~\eqref{eq:defMAP} with expectation $\E[\hat\m_n^\text{MAP}]=:\bar \m_n$ (light green), and MLE is unbiased $\E[\hat\m_n^\text{MLE}]=\m^*$, as illustrated by the parallels in the grey triangle.
	In primal space, MAP has expectation $\tilde \nat_n$ (red), which intervenes in the bias-variance decomposition~\eqref{eq:primal_pivot} from~\S\ref{ssec:bias-variance}.
	The hyperparameter of the prior $\nat_0$ controls the brown point's location while varying $n_0$ spans the Thales triangle and the red curve.
	Large blurry circles in the background are other instances of MAP and MLE revealing their distribution.
	}
	\label{fig:thales}
\end{figure*}

\subsection{Full Gaussian (Non-Trivial)}
\label{ssec:gaussian}
Now that we have solved the case of $\cN(0,\sigma^2)$, consider the full Gaussian $\cN(m,\sigma^2)$, which offers a highly non-trivial example for Problem 1.
Their log-likelihood reads $p(x) = -\frac{(x-m)^2}{2 \sigma^2} - \half \log(2\pi\sigma^2)$.
With sufficient statistic $T(x)=(x, x^2)$,
the mean parameters are $\m = \E[T(X)] = (m , m^2 + \sigma^2)$ belonging to the open set $\cM= \{(u,v) \cond u^2 < v\}$,
and the natural parameters are $\nat= (\frac{m}{\sigma^2} , \frac{-1}{2\sigma^2}) \in \Theta = \real \times \real_-$.
Examples of MAP and MLE  are represented in \cref{fig:thales} within $\cM$ and $\Theta$ delimited in grey.
Given these parameters, log-partition and entropy are, up to constants,
\alignn{
	\textstyle \logpart(\nat) &= \textstyle \frac{\nat_1^2}{-4\nat_2} - \half \log(-2\nat_2) \\
	\textstyle \conj(\m) &= \textstyle - \half \log (\mu_2 - \mu_1^2)
}

These functions are neither smooth, nor strongly convex, nor self-concordant (see App.~\ref{app:gaussian}).
We now discuss the general problem and some ways to solve it via direct expansions of the Bregman divergence.

\section{Partial Solutions}
\label{sec:insights}

\subsection{Asymptotic Rate}
\label{ssec:asymptote}
As a reference point for any finite convergence rate, it is interesting to know the asymptotic behavior of these quantities as $n \rightarrow +\infty$.
We now detail a classical asymptotic result.
Proofs are in App.~\ref{app:asymptote}, and \citet[\S1.1]{ostrovskii2021finite} offers a great review.

Statistics typically give results on $\nat$, but the MAP~\eqref{eq:defMAP} is more simply expressed with $\meanp$, so let us focus on $\bregmanconj$.
Bregman divergences are locally quadratic, as seen via a second order Taylor expansion
\alignn{
    \textstyle \bregmanconj(\m^* ; \m)
    &\textstyle = \frac{1}{2}\norm{\m^* - \m}^2_{\mF}
    + O(\norm{\m - \m^*}^3),
    \label{eq:bregmanTaylor}
}
where the Mahalanobis norm  $\| x \|_{\mF}^2 = x^\top \mF x$  is induced by $\mF  := \nabla^2\conj(\m^*)$, the Hessian of the entropy at the optimum. It happens that  $\mF$ is also the inverse \textit{Fisher information matrix} at $\nat^*$, since
\aligns{
    \mF
    :=\nabla^2\conj(\m^*)
    = \nabla^2\logpart(\nat^*)^{-1}
    = \Cov_{\nat^*}[T(X)]^{-1}  \; .
}
Plugging the MLE~\eqref{eq:defMLE} or MAP~\eqref{eq:defMAP} into~\eqref{eq:bregmanTaylor}, we have
\begin{align}
	\label{eq:asymptote}
	\E \bregmanconj \left (\E [T(X)] ; \hat \meanp_n^\text{MLE/MAP} \right )
	= \frac{d}{2n} + O(n^{- \frac{3}{2}}) \; .
\end{align}
Both MLE and MAP have the same asymptote, as the contribution of the prior $n_0 \meanp_0$ gets negligible for large $n$.
This asymptote is independent of the optimum $\meanp^*$ or $\mF$ for well-specified models.
Next, we give another example for which we get a rate matching~\eqref{eq:asymptote}.

\subsection{Quadratic Case}
\label{ssec:quadratic}
As another classical reference point, we consider the case $\logpart(\nat) = \half \norm{\nat}_2^2$.
For instance, this is the log-partition of a Gaussian with known variance $I$,
\[
	\cX=\real^d,\quad \nu(dx) = \exp\paren{\textstyle \half[-\|x\|^2]} dx,\quad T(x)=x.
\]
In this case, $\conj(\meanp) = \half \norm{\meanp}_2^2$ as well, and both Bregman divergences are squared $\ell^2$ distances since
\begin{align}
	\bregmanconj(\meanp^* ; \meanp) = \half \norm{\meanp^* -  \meanp }_2^2  \; .
\end{align}
Thanks to the independence of samples, we can break down the MLE into individual point's contributions:
\begin{align}
	\expect{\half \norm{\m^* -  \inv{n}  \smallsum_i T_i}_2^2}
	=\frac{\Var(T)}{2n}
	=\frac{d}{2n}.
\end{align}
Adding a reference mean $\m_0$ to get the MAP yields
\begin{align}
		\!\!\!\!\! \expect{\half \norm{\m^* -   \MAPm^\text{MAP}}_2^2} \!
	&= \frac{n \Var(T) +  n_0^2 \norm{\m^* -  \m_0}^2}{2(n+n_0)^2}.\!
	%\\ &= O\left(\frac{\Var(T)}{n} \right) + O\left(\frac{\norm{\m^* -  \m_0}^2}{n^2} \right)
	\label{eq:MAP_quadratic}
\end{align}
We see here a variance term defining the $\frac{d}{2n}$ asymptote and a bias term in $O(n^{-2})$. However, this result does not generalize well to other families unless we make restrictive assumptions on $\conj$. % we can relate $\bregmanconj$ to a norm or a quadratic.
% Can we generalize these results to other families?

{\bf If $\conj$ is $L$-Lipschitz} (e.g. $\logpart$ is defined within the $\ell^2$-ball of radius $L$), then
\begin{align}
    \bregmanconj(\m^* ; \m)
    &\leq L \norm{\m^* - \m} + \norm{\nat} \norm{\m^* - \m} \\
    &\leq 2L \norm{\m^* - \m},
\end{align}
so $\bregmanconj$ is Lipschitz, and~\eqref{eq:MAP_quadratic} yields a $O(\inv{\sqrt{n}})$ rate, but no common exponential families verify the assumption.

{\bf If $\conj$ is $L$-smooth}\footnote{$\conj$ is $L$-smooth iff $\nabla\conj$ is $L$-Lipschitz.} (e.g. $\logpart$ is $\frac{1}{L}$-strongly convex \citep{kakade2009duality}), then
\begin{align}
    \bregmanconj(\m^* ; \m)
    \leq \frac{L}{2} \norm{\m^* - \m}^2,
\end{align}
so $\bregmanconj$ is upper bounded by a quadratic, and we get~\eqref{eq:MAP_quadratic} as an upper bound.
It is also possible to get (more complex) upper bounds under restricted notions of strong-convexity \citep{negahban2012unified}.
Besides the Gaussian with known variance, the problem is that no standard exponential family has a \textit{global} strongly convex log-partition function. The next section focuses on \textit{local} quadratic behavior, which is more realistic.
% So let us focus on a more realistic set of assumptions in the next section.

\subsection{Locally Quadratic Case}
\label{ssec:local-quadratic}
From Taylor expansion~\eqref{eq:bregmanTaylor},
we know that all Bregman divergences are locally quadratic.
Under some assumptions, such as self-concordance\footnote{
In 1d, $f$ is self-concordant iff $\forall x, \abs{f'''(x)} \leq 2 \abs{f''(x)}^{\frac{3}{2}}$.
} of $\conj$ \citep[Ch.~4.1]{nesterov2003introductory}, we can quantify when this quadratic behavior kicks in. Proofs for this subsection are in App.~\ref{app:self-concordant}.
\begin{proposition}
Let $\conj:\cM\rightarrow \real$ be a self-concordant convex function, $\m, \m^* \in\cM$ and $\mF = \nabla \conj(\m^*)$. Then\footnote{$0.21$ is a value of $x$ such that $x^2 \geq -\frac{x}{1-x} - \log(1 - \frac{x}{1-x})$.}
\aligns{
	\norm{\m^*-\m}_{\mF} < 0.21
	\implies
	\bregmanconj(\m^*,\m) \leq \norm{\m^*-\m}_{\mF}^2.
}
\end{proposition}
To gain insights into how many samples are needed, we can estimate when $\expecti{\norm{\m^*-\m}_{\mF}} < 0.21 $.
For the MLE, the proof of~\eqref{eq:asymptote} from~\eqref{eq:bregmanTaylor} yields $\expecti{\norm{\m^*-\hat \m_n}^2_{\mF}} = \frac{d}{n}$ in general, so a sufficient condition is $n \geq 25 d$.
For MAP, transforming~\eqref{eq:MAP_quadratic}, we get the sufficient condition $n\geq 25d + 5 \norm{\m^* -  \m_0} - n_0$.
This means that on average, we need $25$ times more samples than the dimension to reach the quadratic regime and ensure an upper-bound like~\eqref{eq:MAP_quadratic}.

The entropy $\conj$ is self-concordant for several common families.
For instance when $T$ is 1-dimensional and $\logpart$ is self-concordant (App.~\ref{app:self-concordant}),
as is the case with:
exponential distributions,
Gaussians with known mean,
Laplace with known mean,
Pareto with known minimum value,
or Weibull with known shape $k$.
The entropy is also self-concordant when $T$ lives in a compact \citep{bubeck2015entropic} -- e.g., categorical and Dirichlet distributions.

There are quite a few pieces of work in this direction.
\citet{ostrovskii2021finite} characterizes the number of samples needed to reach the asymptotic quadratic behavior for any parametric models with a self-concordant log-likelihood.
\citet{anastasiou2017bounds} obtains a similar flavor of result under other assumptions on the third derivative of $f$.
More closely, in the world of exponential families, \citet{kakade2010learning} assumes a local bound on all higher-order moments of $\logpart$ in $\nat^*$, to obtain a similar result.
However, these results are expressed with quadratics in $\nat$, not $\m$, and they do not directly translate to convergence rates for the MAP, but they might with some more work.

These results partially answer~\eqref{problem}.
Partially because the present proposition and these related works all give \textit{large sample} results,  that hold when $n\geq N$ for some constant $N$.
None of them apply to small $n$.
That is one caveat. Another one is the Gaussian example we saw in \S\ref{ssec:gaussian}, which is not self-concordant.
A full solution to~\eqref{problem} would address both of these caveats.
Informed by the properties we have seen so far, let us investigate a general decomposition of the Bregman that could guide us towards a solution.
%If no current approach solves~\eqref{problem}, then let us formulate a conjecture informed by the examples and partial solutions we have s.


\subsection{Bias-Variance Decomposition}
\label{ssec:bias-variance}
In both the quadratic~\eqref{eq:MAP_quadratic} and the Gaussian variance examples~\eqref{eq:MAP_rate}, the upper bound takes the form $O(\inv{n}) + O(\frac{\text{bias}}{n^2})$,
giving us a flavor of what we would like as a general result for exponential families: a finite sample convergence rate, with variance and bias terms that reflect the important constants of the problem.
Such a decomposition exists for any Bregman divergence \citep[Theorem 0.1]{pfau2013generalized}.
\begin{theorem}[Bregman Bias-Variance Decomposition]
	Let $\tilde \theta_n := \expecti{\hat \theta_n}$ be the expectation of the MAP in primal space, and $\tilde \m_n = \nabla \logpart(\tilde \theta_n )$ be its dual representation. The  expected Bregman decomposes into
\begin{equation}
	\expect{\bregmanconj(\m^* ; \hat \m_n)} = {\bregmanconj(\m^* ; \tilde \m_n)}
	+ {\expect{\bregmanconj(\tilde \m_n ; \MAPm)}}
	\label{eq:primal_pivot}
\end{equation}
\end{theorem}
We plot this decomposition for $\cN(\mu, \sigma^2)$ in Fig.~\ref{fig:gaussian_decomposition} , and we illustrate the primal mean $\tilde \nat_n$ in  Fig.~\ref{fig:thales}.

\textbf{Remark:} In this decomposition, the primal expectation $\expecti{\hat \theta_n}$ is the reference point.
An estimator will be unbiased if $\tilde \nat_n = \nat^*$.
This is not true for the MLE, which is unbiased w.r.t. the dual parameter $\expecti{\hat \m_n}=\m^*$.

We show in App.~\ref{app:bias-variance} that the bias decreases like ${\bregmanconj(\m^* ; \tilde \m_n)} \leq \frac{2}{n(n-2)}$ for Gaussian variance MLE, and
 ${\bregmanconj(\m^* ; \tilde \m_n)} \leq \frac{ \norm{\m^* - \m_0}^2 }{(1 + \frac{n}{n_0})^2 }$ for a quadratic MAP.
These observations hint towards a general $O(1/n^2)$ upper bound for the bias, while the variance may be less dependent on the initialization $\nat_0$.

\begin{figure}[t]
	\centering
	\includegraphics[width=.4\textwidth]{figs/gaussians/new_linear_n0=1.pdf}
	\caption{
	Bias-Variance Decomposition for a Gaussian $\cN(m, \sigma^2)$ with $\meanp^*=(0, 1), \meanp_0 = (1,2)$ and $n_0=1$. The asymptote is $\frac{1}{n}$.
	}
	\label{fig:gaussian_decomposition}
\end{figure}

In this section, we considered direct expansions of~\eqref{eq:bregmanMAP}.
None of them could fully solve~\eqref{problem}.
Next, we investigate whether an optimization approach could solve it.


\section{AN OPTIMIZATION PROBLEM}
\label{sec:optimization}

%This section show how MAP can be interpreted as stochastic mirror descent (SMD). Thanks to this strong link, we show that \textbf{1)} we may obtain a convergence rate for MAP from the rate od SMD, and \textbf{2)} any insights gained from the MAP may inform on further design and analysis of SMD.
%Then, we review the assumptions of relative smoothness, useful to deal with non-smooth functions, before investigating three different analyses of SMD with the MAP.
%\rlp{mention that examples have so far been lacking for relative SMD.}
%
%\subsection{MAP as Stochastic Mirror Descent}
%\label{ssec:MAP=SMD}
%By the Bayes rule, the posterior can always be written iteratively $p(\nat | \mD_n) \propto p(X_n | \nat) p(\nat | \mD_{n-1})$.
%Plugging in~\eqref{eq:joint_likelihood}, and introducing $f_n(\nat) = - \log p(X_{n} | \nat)$, we see that the MAP verifies
%\begin{align*}
%	\hat \nat_{n}
%	= \argmin_\nat f_n(\nat) + (n_0 + n -1) \bregman(\nat; \hat \nat_{n-1})
%    %\label{eq:SBPP}
%\end{align*}
%which is the update of a Stochastic Bregman Proximal Point update with step-size $\inv{n_0 + n - 1}$.
%%Maybe skip this SBPP story altogether
%If we expand $f_n(\nat) = \logpart(\nat) - T_n$,  we get (after some manipulation with the Bregman divergence)
%\begin{align*}
%		\hat \nat_{n}
%	= \argmin_\nat \langle g_{n}(\hat \nat_{n-1}), \nat \rangle + (n_0 + n) \bregman(\nat; \hat \nat_{n-1})
%    %\label{eq:SMD}
%\end{align*}
%where $g_{n}(\nat) := \nabla f_n(\nat) = \hat \m_{n-1} - T_n$.
%We recognize the update of Stochastic Mirror Descent (a.k.a. Stochastic Bregman Gradient) with step-size $\lr_n = \inv{n_0 + n}$.
%From the first order optimality condition, we see this is a stochastic gradient step in the dual $\hat \m_n = \hat \m_{n-1} - \lr_n g_n$.
%This parallel means we could obtain a finite convergence rate for the  MAP from an analysis of these algorithms.
%In particular, it could potentially apply to a wide range of distribution $\cD$ on $X$.
%Note that this optimization perspective does not hold for the MLE %($n_0=0$)
%as its implicit uniform prior may correspond to an initialization outside of $\Theta$.


As we saw in \S\ref{sec:problem}, MAP can be interpreted as stochastic mirror descent (SMD).
This means that \textbf{1)} we may obtain a convergence rate for MAP from an optimization analysis, and \textbf{2)} any insights gained from the MAP may inform other designs and analyses of SMD.
We first review the assumptions of relative smoothness, helpful to deal with non-smooth functions, before investigating recent analyses of SMD with the MAP.

\subsection{Relative Smoothness}
Mirror descent (MD) \citep{nemirovski1983problem,beck2003mirror}, also known as
Bregman (proximal) gradient, relative gradient descent or NoLips,
%Nemirovski introduced the algorithm, and Beck established the connection with Bregman divergences, e.g. the argmin form.
and SMD \citep{nemirovski2009robust,ghadimi2012optimal}
are typically encountered in non-smooth (online) optimization,
under bounded (or Lipschitz) gradient assumption on the objective $f$
and strong convexity assumption on the potential $\logpart$
\citep[Th. 4.2(MD) \& Th. 6.3(SMD)]{bubeck2015convex}.
In our case, these assumptions do not hold.
For instance $\logpart = -\log$ is neither smooth nor strongly convex.

Recently, these assumptions have been relaxed to the $\stgcvx$-strong convexity and $\smooth$-smoothness of $f$
\emph{relative} to a reference function $\logpart$, defined as
\aligns{
	\stgcvx \cB_{A}(x, y)
	\leq
	\cB_f(x,y)
	\leq
	\smooth \cB_A(x,y) \; .
}
When $\logpart = \norm{\cdot}^2$, we recover the standard smoothness and gradient descent.
These conditions ensure the linear convergence of MD with mirror map $\nabla A$
\citep{birnbaum2011distributed, bauschke2017descent, lu2018relatively},
even when $f$ is not smooth, and $\logpart$ not strongly convex.

For exponential families, MAP perfectly fits into this framework, as
\aligns{
	f(\theta) = A(\theta) - \expect{\lin{T(X), \theta}}
}
is $1$-smooth and $1$-strongly convex relative to $A$.
Our goal is then to find an applicable convergence rate for SMD under relative smoothness.

\subsection{Bounding the Randomness}

To analyze stochastic algorithms, one also needs to quantify the randomness of stochastic gradients $g(\nat)$.
While many assumptions exist for SGD \citep[\S3 for a modern review]{khaled2020better}, only a few have been adapted to SMD with relative smoothness \citep{hanzely2018fastest, dragomir2021fast, dorazio2021stochastic}, but they have so far been lacking concrete examples.
We review these analyses in the light of the MAP and provide a summary in \cref{tbl:assumptions}.

\begin{table}[t]
	\newcommand*{\greencmark}{\textcolor{Green}{\cmark}}
	\newcommand*{\redxmark}{\textcolor{Red}{\xmark}}
	\caption{Summary of results for SMD
		under relative smoothness and relative strong convexity assumptions.
		Each row correspond to one analysis, and each columns answers one question.
		($-\log$) does the bound hold for the Gaussian variance example (\S\ref{ssec:gaussian-variance})?
		($\lr_n \sim \inv{n}$) does it converge with a $O(\inv{n})$ step-size?
		($f$) is the bound in function value, or in reverse Bregman $\bregman(\nat^*,\hat\nat_n)$?
		($\hat\nat_n$) is it for the last iterate or an average ?
		None of these analysis check all the boxes needed to address~\eqref{problem}.
	}
	\begin{center}
		\begin{tabular}{lcccc}
			%{p{.15\textwidth}m{.05\textwidth}m{.08\textwidth}m{.1\textwidth}}
			\toprule
			Boundedness & $-\log$ &  $\lr_n \sim \inv{n}$ & $f$ & $\hat\nat_n$ \\
			\midrule
			%Strongly convex and bounded variance\newline
			%\citep[e.g.,][]{??}
			%&
			%$
			%\cB_A(\theta,\theta') \geq \frac{1}{2}\norm{\theta-\theta'}^2,
			%\quad
			%\expect[x]{\norm{\m - x}}^2\leq\sigma^2
			%$
			%\\
			Var. on $\Theta$~\eqref{eq:hanzely} % \newline \citep{hanzely2018fastest}
			& \redxmark & \greencmark & \greencmark  & \redxmark
			\\
			Var. at $\theta_*$~\eqref{eq:dragomir} %\newline \citep{dragomir2021fast}
			& \redxmark & \greencmark & \redxmark  & \greencmark
			\\
			Opt. gap~\eqref{eq:dorazio} %\newline \citep{dorazio2021stochastic}
			& \greencmark & \redxmark & \redxmark & \greencmark
			\\
			\bottomrule
		\end{tabular}
	\end{center}
	\label{tbl:assumptions}
\end{table}


%Before we proceed, let $f_X$ be a stochastic estimate of $f = \expecti{f_X}$, with stochastic gradients $g_X(\nat) =\nabla f_X(\nat)$. In our case $f_X(\nat) = - \log p(X\cond \nat)$ and $g_X(\nat) = \m - T(X)$.

\subsubsection{Analogs of the Variance}
Let us introduce the symmetrized Bregman induced by $\conj$, written $\cS_\conj(\m_1, \m_2) = \bregmanconj(\m_1, \m_2) + \bregmanconj(\m_2, \m_1)$.
\citet{hanzely2018fastest} assume that the expectation of $\cS$ between stochastic and deterministic updates verifies
\alignn{
	\expect[g]{\cS_\conj \paren{
			\hat\m_{n} - \lr g(\hat \nat_{n}),
			\hat\m_{n}   -\lr \nabla f(\hat \nat_{n})
	}} \leq \lr^2 C
	\label{eq:hanzely}
}
for all possible iterates $\MAPt$, relevant step-sizes $\lr$ and for some constant $C$.
When $A(\theta) = \frac{1}{2}\norm{\theta}^2$ or when $A$ is $\stgcvx$-strongly convex,
this definition recovers the variance of the stochastic gradient
\[
\expect[g\!\!]{\inv{\stgcvx}\norm{\nabla f(\theta) - g(\theta)}^2}\leq C.
\]
Under this assumption, \citet[Lem.4.8]{hanzely2018fastest} prove a $O(1/n)$ convergence rate on function values with $O(1/n)$ step-sizes and tail averaging \citep{lacostejulien2012simpler} in primal space $\Theta$.

\citet{dragomir2021fast} define the weaker assumption
\alignn{
	\expect[\tilde g]{
		\cB_{A^*}(\MAPm - 2\lr g(\theta_*), \MAPm)
	} \leq 2 \lr^2 C \; .
	\label{eq:dragomir}
}
When $\logpart$ is $\stgcvx$-strongly-convex, we recover the variance of the gradients at the optimum
\[
\expect[g\!\!]{\inv{\stgcvx}\norm{\nabla g(\theta^*)}^2}\leq C.
\]
Using their descent lemma \citep[Eq. (12)]{dragomir2021fast} with the $O(1/n)$ step-size used by \citet[Th. 3.2]{gower2019sgd} for SGD, we obtain a $O(1/n)$ convergence rate, on the Bregman with \emph{reversed} arguments $\bregman(\nat^*, \hat\nat_n)$.

These two analyses seem promising for~\eqref{problem}, but none of these assumptions hold in front of barrier objectives such as the $-\log$ from \S\ref{ssec:gaussian-variance}.
Indeed, they both assume their bound holds uniformly for every possible iterate $\hat \nat_n$.
Yet $\cN(0,\sigma^2)$ has a positive mass around $0$.
This means that $\hat \m_n$ can get arbitrarily close from $0$, where the $-\log$ is unbounded, along with the associated  Bregman divergences~\eqref{eq:hanzely} and~\eqref{eq:dragomir}.
In general, this uniform bound over $\hat \m_n$ cannot hold for \emph{barrier} objectives -- functions exploding to infinity in some finite point of space.

However, both of their proofs hold if we add an expectation over $\MAPm$ to their assumption.
However, this is not helpful, as verifying the assumption becomes as hard as the initial problem.
For instance, the expectation of~\eqref{eq:hanzely} over $\MAPm$ is an upper bound on the variance term of~\eqref{eq:primal_pivot}.
Confronted with this difficulty, we investigate an alternative definition of variance. 

\subsubsection{Bounded Optimality Gap}
Inspired by \citet{loizou2021stochastic}, \citet{dorazio2021stochastic} explore the hypothesis
\alignn{
	\min_\nat f(\nat) - \expect[X]{\min_\nat f_X(\nat)} \leq C,
	\label{eq:dorazio}
}
where $f_X$ is a stochastic estimate of $f = \expecti{f_X}$. In our case $f_X(\nat) = - \log p(X\cond \nat)$.
In other words, this lower bounds the expectation of the minimum of the stochastic estimates.
For probabilistic models, such a bound is finite as soon as the model cannot give infinite density to any data point $x$.
This holds, for instance, for discrete distributions because the probability mass is upper bounded by $1$; however, it rules out many families.
In the case of normal distributions $\cN(m, \sigma^2)$, setting $m=x$ and $\sigma^2 \rightarrow 0$ gets $p_\nat (x) \rightarrow +\infty$. We have a similar behavior for gamma distribution with $\alpha = \beta x$ and $\beta \rightarrow +\infty$, or with the beta distribution with $\alpha=\beta \frac{x}{1-x}$ and $\beta \rightarrow +\infty$.
Other counter-examples include inverse Gaussians, log-normal, gamma, inverse gamma.

It is possible to overcome this limitation by treating batches of samples as single samples by averaging sufficient statistics, e.g.,. $Y = \{X_1, \dots, X_k\}$ and $T(Y) = \inv{k}\sum_i T(X_i)$.
For instance, a multivariate normal of dimension $d$ cannot attribute infinite density to $d+1$ samples that are not in an affine subspace.

Overall, \eqref{eq:dorazio} can partially handle barrier objectives, but it fails to account for the step-size $\lr_n = \inv{n_0+n}$, as \citet[Thm.1]{dorazio2021stochastic} only proves linear convergence to a variance ball of size $\frac{C}{\stgcvx}$ under constant step-size.
This is in contrast with~\citet{dragomir2021fast} which can handle decreasing step-sizes but not barrier objectives.
Proving convergence of stochastic mirror descent on barrier loss remains an open problem.

\section{Conclusion}
Despite the MLE and MAP estimators in the exponential family being classical and known in statistics for decades, we highlighted in this paper open problems to bound their frequentist risk (the expected KL) in a non-asymptotic way. We reviewed some partial results, such as a large sample analysis that describes how many samples are needed to ensure a locally quadratic regime~\citep{kakade2010learning, ostrovskii2021finite} for which rates are known. We also related this problem to the one of obtaining convergence rates in stochastic optimization, observing that MAP fits the framework of stochastic mirror descent with relative smoothness assumptions. 
Nevertheless, none of the current analyses of SMD hold for the MAP, even on a simple family such as $\cN(0,\sigma^2)$, thus revealing an area for progress in non-Euclidean optimization.
In writing this paper, we hope to attract attention to this fundamental problem, leading to progress in both optimization and statistics.



\newpage
\bibliographystyle{apalike}
\bibliography{references}


\clearpage
\appendix
\onecolumn

\section{PROOFS FOR GAUSSIAN VARIANCE}
\label{app:gaussian-variance}

Intuitively, the divergence measures the discrepancy between the ratio $\frac{ \nat_n}{\nat_*} =  \frac{\m_*}{ \m_n}  $ and $1$ via the function
\begin{align}
	\phi(z) := \half (z - 1 - \log(z))
\end{align}
illustrated in Figure~\ref{fig:phi}.
\begin{figure}[ht]
	\centering
	\includegraphics[width=.4\textwidth]{phi.pdf}
	\caption{$\phi(z)$ is the Bregman divergence induced by $-\log(z)$. It is a barrier near $0$. As a result, it admits another upper bound (in grey) despite being poorly approximated by quadratics.}
	\label{fig:phi}
\end{figure}

For MAP, we get an upper bound thanks to
\begin{align}
	\label{eq:log_bound}
	\phi(z) \leq \phi(z) + \phi(\inv{z}) =  \half (z + \inv{z}) - 1 \; .
\end{align}

\section{PROOFS FOR GAUSSIAN}
\label{app:gaussian}

\section{ASYMPTOTIC DERIVATION}
\label{app:asymptote}

\section{SELF-CONCORDANCE}
\label{app:self-concordant}

\section{BIAS-VARIANCE}
\label{app:bias-variance}

\section{REVIEW OF SMD}
\label{app:SMD}

\section{TODO}
\begin{enumerate}
	\item new plots: add 2d trajectories to paper, with background level set, and new legend: primal/dual expectation, and clear explanation!
	\item Incorporate related work by Frederik.
	\item Technical Background: be precise and add a reference for the steep exponential family of Legendre type, which guarantees that our sets are open and have a solution in the middle.
	References: \begin{itemize}
		\item \href{https://www.jstor.org/stable/4616462?seq=1#metadata_info_tab_contents}{Existence of Maximum Likelihood Estimates for Multi-Dimensional Exponential Families},
		\item The singly truncated normal distribution: A non-steep exponential family,
		\item Statistical exponential families: A digest with flashcards,
		\item Information and Exponential Families In Statistical Theory,
		\item Fundamentals of Statistical Exponential Families with Applications in Statistical Decision Theory.
	\end{itemize}
\end{enumerate}




\section{Blurbs/ideas?}



\subsection{SGD blurb}
\fdk{
%
There is a long line of work bounding the convergence rate of stochastic gradient descent.
Beyond the results of \citet{robbins1951stochastic},
we now have proofs on smooth, strongly convex problems and bounded variance
(as well as other assumptions on the noise, reviewed later)
a decreasing step-size and an averaging scheme gets
a $1/t$ convergence rate,
which matches the asymptotic rate of unbiased estimation,
through clever averaging schemes \citep{rakhlin2012making,lacostejulien2012simpler}
\\
Those papers assume the stochastic gradients are bounded,
but minor modifications also work for bounded variance instead.
\\
We do have extensions to other notions of bounded variance,
for example, assuming that the stochastic gradients are gradients of a perturbed smooth function $f_i$
and that the minimum of $f$ and the minima of the $f_i$
are bounded, $\expect{\min_x f(x) - \min_x f_i(x)} \leq \sigma^2$,
or that the gradient noise is bounded only at the minimum.
For a review, see \citet{gower2019sgd}.
\\
However, for some problems, combining decreasing step-sizes and averaging is not necessary,
even when the function is not strongly convex.
This is the case for maximum likelihood estimation and matches the asymptotic rates,
but holds more generally, for example, for linear and logistic regressions \citep{bach2013nonstronglyconvex,moulines2011non}.
}


\subsection{Poisson likelihood?}
\fdk{
\citet{bauschke2017descent} and \citet{hanzely2018fastest} both use the example of Poisson inverse problems/Poisson regression
as examples.
The simpler case of the MLE of a Poisson distribution is also unsolved, though.
In this case, $h(x) = 1/x!$, $x \in \mathbb{N}$, $A(\theta) = e^\theta$ over $\theta \in \mathbb{R}$,
$A^*(\m) = \m \log \m - \m$ over $\m \in \mathbb{R}_+$.
It is neither strongly-convex nor self-concordant (at least according to the standard definition,
although it satisfies generalized notions of self-concordance as $A'''(\theta) = A''(\theta)$).
}

\subsection{``simple'' open problem?}
\fdk{
A ``simple open problem'';
assume we have a deterministic problem so that we know where the minimum is.
Can we figure out a (deterministic) path from $\theta_0$ to $\theta_*$
that has optimality decreasing as $1/n^2$,
to mirror the bias term of gradient descent?
}

\subsection{Connection with acceleration?}
\fdk{
The problems on how to deal with Bregman divergences abound in optimization, beyond stochasticity.
For example, we have not yet figured out the analog of Nesterov-type acceleration
on relatively smooth and strongly convex problems
to bring the convergence rate from linear in $(1-\kappa)$ to $(1-\sqrt{\kappa})$,
or just from $1/T$ to $1/T^2$ in the (non-strongly) convex case.
\citet{dragomir2021optimal}  shows that nave application of Bregman updates can not achieve acceleration.
The tools developed to make progress on one problem might help make progress on the other.
}






\newpage
\section{Notation convention}
We use
\begin{itemize}
	\item $\theta, \nat$ for natural parameters
	\item $\m$ for mean parameters
	\item $X_1,\ldots,X_n$ for data
	\item $\cX$ for the data space
	\item $T$ for the sufficient statistics
	\item $\nu$ for the base measure
	\item $A, A^*, \logpart, \conj$ for the log-partition and its dual
	\item $\cB_A$ for the Bregman divergence induced by $A$
	\item $n$ for the number of samples
	\item $(\theta^*,m^*)$ for the optimum
	\item $\gamma, \lr$ for the step-size
	\item $\mu,\sigma^2$ for the parameters of a Gaussian
	\item $\hat\theta_n$ for estimates
	\item $\tilde\theta_n$ for $\expect{\hat\theta_n}$, which is biased
	\item $(\stgcvx , \smooth)$ for strong-convexity and smoothness
\end{itemize}

 \end{document}