\documentclass{article}

\usepackage[top=2cm, bottom=2cm]{geometry}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{hyperref}       % hyperlinks
\usepackage[dvipsnames]{xcolor}
\usepackage{tikz}           % define for loops

\hypersetup{ % SLJ: my standard paper setup...
	pdftitle={MAP convergence},
	pdfkeywords={},
	pdfborder=0 0 0,
	pdfpagemode=UseNone,
	colorlinks=true,
	linkcolor=blue, %mydarkblue,
	citecolor=blue, %mydarkblue,
	filecolor=blue, %mydarkblue,
	urlcolor=blue, %mydarkblue,
	pdfview=FitH,
	pdfauthor={Anonymous},
}

\usepackage[round]{natbib}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsection*{\bibname}}

\newcommand{\RLP}[1]{\textcolor{red}{RLP:#1}}
\newcommand{\violet}[1]{\textcolor{violet}{#1}}
\newcommand{\TODO}[1]{\textcolor{cyan}{TODO #1}}

% frames with colored background
\definecolor{light-gray}{gray}{0.85}
\definecolor{myblue}{RGB}{199, 235, 255}
\usepackage[framemethod=TikZ]{mdframed} 
\usetikzlibrary{shadows}

\newmdenv[% box with grey background
    backgroundcolor=gray!15,
    linewidth=0pt,
    roundcorner=5pt,
    shadow=true,
	shadowsize=4pt
]{example}
\newmdenv[
	backgroundcolor=myblue,
	roundcorner=5pt, 
	linewidth=0pt, 
	shadow=true,
	shadowsize=4pt
]{important}

% my packages
\usepackage{math_commands}
% some custom math commands
\newtheorem{proposition}{Proposition}
\newcommand*{\expect}[2][]{\ensuremath{\mathbb{E}_{#1} \left[ #2 \right] }} % expectation operator
\newcommand{\logpart}{A}
\newcommand{\conj}{\logpart^*}
\newcommand{\bregman}{\cB_\logpart}
\newcommand{\bregmanconj}{\cB_{\logpart^*}}
\newcommand{\natp}{\theta}
\newcommand{\meanp}{\mu}
\newcommand{\decrement}{D}
\newcommand{\Dir}{\mathrm{Dir}} % Dirichlet
\newcommand{\linear}{\ell} % linearization of a function
\newcommand{\lr}{\gamma} % learning rate, or step-size

\newcommand{\MAPm}{\hat \mu_n}
\newcommand{\MAPt}{\hat \natp_n}


\title{Convergence rate of MAP estimates \\
for the exponential family}
\author{R\'emi Le Priol}
\date{October 2020}

%% Faster processing
\renewcommand\appendix{\end{document}}


\begin{document}

\maketitle

\section{Background}

\paragraph{Motivation.} 
We do not know general convergence rates on the KL for maximum likelihood estimates of the exponential family.
We want the simplest one.
We hope  to get a new result by combining tools from statistics and optimization. 
\paragraph{Exponential Family.}
The exponential family member with sufficient statistic $T$ and natural parameter $\natp$ is the model 
\begin{equation}
	 p(X|\natp) = \exp( \natp^\top T(X) - \logpart(\natp)) \; ,
\end{equation}
where $\logpart$ is the log-partition function (aka normalization factor) 
\begin{align}
    \logpart(\natp) = \log \int e^{\natp^\top T(x)} dx \; .
\end{align}

\begin{example}
	These frames contain the trailing example of this paper: a centered gaussian with unknown variance $\cN(0,\sigma^2)$.
	The density of a centered normal variable is
\begin{align}
	p(x) = \inv{\sqrt{2\pi \sigma^2}} e^{-\frac{x^2}{2 \sigma^2}} \; .
\end{align}
Defining $T(X)=X^2$ as the sufficient statistic, we get natural parameter $\natp = -\inv{2 \sigma^2} <0$, and mean parameter $\mu=\E[T(X)] = \sigma^2 >0$. 
Mean and natural parameters are roughly inverse of each other
\begin{align}
	\natp = -\inv{2 \mu}\; .
\end{align}
Now we can match the log-likelihood with the exponential family template to get the log-partition function.
\begin{align}
	\log p(x) = - \frac{x^2}{2 \sigma^2} - \half \log(2 \pi \sigma^2 ) 
	= x^2 \natp - \logpart(\natp) \\
	\implies \logpart (\natp) = - \half \log(-\natp)  + \half \log(\pi) 
\end{align}
\end{example}

\paragraph{Duality}
The logpartition function $\logpart$ verifies the two following identities
\begin{align}
    \nabla\logpart(\natp) &=  \expect[p(X|\natp)]{T(X)} =: \meanp \\
    \nabla^2 \logpart(\natp) &= \Cov_\natp[T(X)] > 0
\end{align}
where $\meanp$ is called the mean parameter.
If the sufficient statistic $T$ is minimal, then the log-partition function $\logpart$ is strictly convex and its gradient $\nabla \logpart$ is a bijection between natural parameters $\natp$ and mean parameters $\mu$.
The second identity entails that $\logpart$ is strictly-convex. 
At this point it is useful to introduce the \href{https://en.wikipedia.org/wiki/Convex_conjugate}{convex conjugate} (aka Fenchel-Legendre transform) of the logpartition function
\begin{align}
	\conj(\mu) = \langle \mu, \natp \rangle - \logpart(\natp) \; .
\end{align}
It turns out that $\conj$ matches the common notion of \textit{entropy} in information theory, so we will call it entropy.
If $\logpart$ is strictly convex, then its gradient is strictly monotone, so it is a bijection, and its inverse is the gradient of its dual $\nabla\conj \circ \nabla\logpart(\natp) = \natp$ (cf Fig.~\ref{fig:duality}).
For a full review of exponential families and their duality, see \citet[Chapter 3]{wainwright2008graphical}.
\begin{figure}[ht]
	\centering
	\includegraphics{figs/duality}
	\caption{The gradient of the log-partition function and its dual, $(\nabla \logpart, \nabla \conj)$, form a bijection between the natural and mean parameters $\natp, \meanp$. Figure reproduced from \citet{kunstner2020homeomorphic}.}
	\label{fig:duality}
\end{figure}
\begin{example}
For $\cN(0,\sigma^2)$,we can use the formula $\conj(\mu) = \mu \natp - \logpart(\natp)$ to get the entropy
\begin{align}
	\conj(\mu) = \half\left( -\log(\mu) + \log\frac{\pi}{2} - 1 \right) \; .
\end{align}
We can also take gradient and derivative of $\logpart(\natp)$ to retrieve the mean and covariance of the sufficient statistic $X^2$
\begin{align}
	\nabla\logpart(\natp) &= \frac{-1}{2 \natp} = \sigma^2 = \mu = \E[X^2] \\
	\nabla^2\logpart(\natp) &= \frac{1}{2 \natp^2} = 2 \sigma^4 = 2 \mu^2 = \Var(X^2) 
\end{align}
which we confirm thank to wikipedia since $\E[X^4] = 3 \sigma^4$ and thus $\Var(X^2) = \E[X^4] - \E[X^2]^2 = 3 \sigma^4 - \sigma^4 = 2 \sigma^4$.
\end{example}

\paragraph{Conjugate Prior}
The conjugate prior for $p(X|\natp)$ is
\begin{align}
    \pi(\natp) &\propto \exp( - n_0 \bregman(\natp ; \natp_0) )
\end{align}
where $n_0$ and $\natp_0$ are (hyper)parameters of the prior, and $\bregman(\natp ; \natp_0)$ is the Bregman divergence induced by $\logpart$ between $\natp$ and $\natp_0$
\begin{align}
    \bregman (\natp ; \natp_0)
    & = \logpart(\natp) - \logpart(\natp_0) 
    - \langle \nabla \logpart(\natp_0)  , \natp - \natp_0 \rangle
\end{align}
with $\nabla \logpart(\natp_0) = \expect[\natp_0]{T(X)} =: \meanp_0$ the mean parameter associated to $\natp_0$. 
Intuitively, $n_0$ is a number of fictive points observed from a distribution with parameter $\natp_0$.
We can re-write this prior as 
\begin{align}
    \pi(\natp) \propto 
    \exp( -n_0 \logpart (\natp) 
    + \langle n_0 \mu_0, \natp \rangle ) \; ,
\end{align}
which is the formula for the exponential family with sufficient statistics $(\natp ,\logpart(\natp))$ and with natural parameter $(n_0 \mu_0, -n_0)$.
The posterior given $\cD=(X_1,\dots, X_n)$ is then part of the same family, with natural parameters $(n_0 \mu_0 + \sum_i T(X_i) , -(n_0 + n))$.

\begin{example}
In the case of $\cN(0,\sigma^2)$, the logpartition function is $\logpart(\natp) = -\log(-\natp) /2 + \cst$, thus the conjugate prior is the exponential family with sufficient statistic $(\natp, \log(-\natp) )$, eg a negative \href{https://en.wikipedia.org/wiki/Gamma_distribution}{Gamma distribution}.
In particular,
\begin{align}
	\pi(\natp) 
	&\propto 
    \exp( -n_0 \logpart (\natp) 
    + \langle n_0 \mu_0, \natp \rangle ) \\
    &\propto \exp( \half[n_0] \log(-\natp) + n_0\mu_0 \natp ) \\
	&\propto (-\natp)^{1 + \half[n_0] -1 } e^{-n_0 \mu_0 (-\natp)} / Z
\end{align}
from which we infer the shape parameter $\alpha=1 + \half[n_0]$ and the rate parameter $\beta = n_0 \mu_0$, eg $\natp \sim \Gamma (1+\half[n_0] , n_0 \mu_0)$. After seeing $n$ samples, the posterior is $\Gamma\left(1+\half[n_0+n] , n_0 \mu_0 + \sum_i T(X_i) \right)$.
\end{example}

\paragraph{Maximum A Posteriori (MAP).}
The negative log-likelihood of the prior is
\begin{align*}
    -\log \pi(\natp) = n_0 (\logpart(\natp)  - \natp^\top \meanp_0 ) + \cst
\end{align*}
Thus the joint log-likelihood of $\cD =(X_1,\dots,X_n,\natp)$ is
\begin{align}
    -\log p(\cD|\natp)\pi(\natp) 
    = (n_0+n) \logpart (\natp) 
    - \theta^\top \left(n_0 \meanp_0 + \sum_{i=1}^n T(X_i) \right) + \cst \; .
\end{align}
Minimizing this expression over $\natp$ yields the Maximum A Posteriori estimate
\begin{align}
    \hat \natp = \argmin_\natp -\log p(\cD|\natp) + n_0 \bregman(\natp ; \natp_0)
\end{align}
such that the MAP is
\begin{align}
    \nabla \logpart(\hat \natp_\text{MAP}) = \hat \meanp_\text{MAP}
    = \frac{n_0 \meanp_0 + \sum_{i=1}^n T(X_i) }{n_0+n} \; .
\end{align}
When $n_0=0$ -- eg we observed zero samples from the prior -- we recover the Maximum Likelihood Estimate (MLE)
\begin{align}
	\hat \mu_\text{MLE} = \frac{\sum_{i=1}^n T(X_i)}{n}
\end{align}
The MLE and MAP estimates are statistics of the dataset $\cD$. Given a random dataset, we wish to bound their deviation from the optimum $\natp^*$ or $\meanp^*$.


\section{Problem}
For a well-specified model, the suboptimality on the population log-likelihood is exactly the KL between our current model and the true distribution
\begin{align}
    \expect[X\sim p(.|\natp^*)]{-\log p(X|\natp) + \log p(X|\natp^*) }
	= \KL( p(.|\natp^*) ; p(.|\natp)) \; .
\end{align}
For the exponential family, the KL is also the Bregman divergence induced by the log-partition function (with switched arguments)
\begin{align}
	\KL( p(.|\natp^*) ; p(.|\natp)) 
	= \bregman (\natp ; \natp^*)  \; .
\end{align}
There is a general relationship between Bregman divergences and convex conjugates (notice the argument switching)
\begin{align}
	\bregman (\natp ; \natp^*)
    = \logpart(\natp) - \langle \natp , \mu^* \rangle + \conj(\mu^*)
    = \bregmanconj ( \meanp^* ; \meanp)
\end{align}
so in the end the suboptimality is a divergence, which can either be seen as a KL between distributions, as a divergence between natural parameters, or as a divergence between mean parameters
\begin{align}
\boxed{
	\KL( p(.|\natp^*) ; p(.|\natp))
    = \bregman (\natp ; \natp^*)
    = \bregmanconj ( \meanp^* ; \meanp) \; .
}
\end{align}
\begin{important}
The question is: how does this quantity behave when $\natp$ is the maximum-likelihood or the MAP estimate ? Can we get bounds on the following quantities
\begin{align}
	\label{eq:bregmanMLE}
	\expect[X_i\sim \natp^*]{\bregmanconj \left (\E [T(X)] ;  \inv{n}  \smallsum_i T(X_i) \right )} \leq \ ? \; , \\
	\label{eq:bregmanMAP}
	\expect[X_i\sim \natp^*]{\bregmanconj \left (\E [T(X)] ; \frac{n_0 \mu_0 + \smallsum_i T(X_i)}{n_0+n} \right )} \leq \ ? \; ,
\end{align}
where the outer expectation is on the dataset $X_1, \dots, X_n$?
\end{important}

\paragraph{Remark.}
What we are looking for is really akin to concentration inequality, expressed with a Bregman divergence instead of a norm. A key difference though, is that the random variable $T(X)$ is connected to the metric $\logpart$. Indeed expressions~\eqref{eq:bregmanMLE} or~\eqref{eq:bregmanMAP} can be infinite for another choice of random variable. For instance, if we plug in $\conj(\mu)= -\log(\mu)$, which defines a divergence on positive numbers, and $T(X) \sim \cN(0,1)$ which can be negative.

\begin{example}
	For $\cN(0,\sigma^2)$, both the entropy and the log-partition are roughly negative logarithm $z\mapsto - \log(z)$. Which yields the same shape of Bregman divergence, as visible below (all three lines are equal)
\begin{align}
	\KL( \sigma_*^2 ; \sigma_n^2 ) 
	&= \half \left ( \frac{\sigma_*^2}{ \sigma_n^2} - 1 - \log \frac{\sigma_*^2}{ \sigma_n^2} \right) \\
	\bregmanconj( \mu_*; \mu_n) 
	&= \half \left ( \frac{\mu_*}{ \mu_n} - 1 - \log  \frac{\mu_*}{ \mu_n} \right) \\
	\bregman( \natp_n; \natp_* ) 
	&=  \half \left ( \frac{ \natp_n}{\natp_*} - 1 - \log  \frac{ \natp_n}{\natp_*} \right) \; .
\end{align}
In other words, this divergence measures the discrepancy between the ratio $\frac{ \natp_n}{\natp_*} =  \frac{\mu_*}{ \mu_n}  $ and $1$ via the function $\phi$
\begin{align}
	\phi(z) := \half (z - 1 - \log(z)) \\
	\bregman( \natp_n; \natp_* )   = \phi(\frac{ \natp_n}{\natp_*}) =  \phi(\frac{\mu_*}{ \mu_n})
\end{align}
as illustrated in Figure~\ref{fig:phi}. We can get a non-transcendental upper bound thanks to the inequality
\begin{align}
	1 - \inv{z} &\leq \log(z) 
	\label{eq:log_bound}\\
	\implies \phi(z) &\leq \half (z + \inv{z}) - 1 = \frac{(z-1)^2}{2 z}
\end{align}
as illustrated in Figure~\ref{fig:phi}.
\end{example}

\begin{figure}[ht]
	\centering
	\includegraphics[width=.4\textwidth]{figs/bregmandef.pdf}
	\includegraphics[width=.4\textwidth]{figs/phi.pdf}
	\caption{$\phi(z)$ is the Bregman divergence induced by $-\log(z)$. It is a barrier near $0$, so it is poorly approximated by quadratics.}
	\label{fig:phi}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[width=.54\textwidth]{figs/asymptote.pdf}
	\includegraphics[width=.43\textwidth]{figs/fewsamples.pdf}
	\caption{Suboptimality of a Gaussian variance MLE against number of samples $n$. Bold curve is average over 100 trials,  dark shaded area is 90\% (dark) confidence interval, light shade is min-max interval.  
	\textbf{Left:} as $n$ increases, the suboptimality matches the $1/2N$ asymptote.
	\textbf{Right:} the first few samples significantly deviate from this behavior. In fact, for $n=1$ and $n=2$, the expected value is infinite, but we have a closed form solution and a simple upper-bound for $n>2$.
	}
	\label{fig:curves}
\end{figure}


\begin{example}
	\begin{theorem}[MLE tight upper bound]
	The MLE of $\cN(0,\mu_*)$ is $\hat \mu_n^\text{MLE} = \inv{n} \sum_i X_i^2 $.
	Its expected suboptimality is infinite when $n\leq 2$, and otherwise upper-bounded as
	\begin{align}
		 \expect{\bregmanconj( \mu_*; \hat \mu_n^\text{MLE}) }
			\leq \inv{2n} +\frac{2}{n(n-2)} \; .
			\label{eq:MLE_rate}
	\end{align}
\end{theorem}
Note that this upper bound is asymptotically tight.
 There is also a closed form for the multivariate generalization, thanks to the \href{https://en.wikipedia.org/wiki/Inverse-Wishart_distribution}{inverse Wishart distribution} and the \href{https://en.wikipedia.org/wiki/Wishart_distribution#Log-expectation}{expectation of the log-determinant of a Wishart}. The expected value is infinite whenever $n \leq d+1$ where $d$ is the dimension. We do not perform these calculations for now as we want to focus on the simplest possible examples.

As for the MAP, we did not manage to get an asymptotically tight upper bound. Nevertheless, using inequality~\eqref{eq:log_bound}, we did get an interesting upper bound. 
To start, let us recall that the MAP of $\cN(0,\mu_*)$ is $\hat \mu_n = \frac{n_0 \mu_0 + \sum_i X_i^2}{n_0 + n}$.
Its expectation is simply $\mu_n= \frac{n_0 \mu_0 + n \mu^*}{n_0 + n}$.
We now present a lemma on its expected inverse.
\begin{lemma}[Expected MAP natural parameter]
	For any $n\geq 2$, the expectation of the natural parameter of the MAP of $\cN(0,\mu_*)$ is bounded as
	\begin{align}
		\frac{\mu^*}{\mu_n}
		\leq \expect{\frac{\mu^*}{\MAPm}} 
		= \expect{\frac{\MAPt}{\natp^*}} 
		\leq \frac{1}{\frac{\mu_n}{\mu^*} - \frac{2}{n+n_0}} \; .
	\end{align}
\end{lemma}
Not that the lower bound is a simple consequence of the convexity of the inverse function. Using this lemma, along withthe los upper bound on the logarithm~\eqref{eq:log_bound}, we get a convergence rate for the MAP. We relate this rate convergence rate with the rate of MLE in Figure~\ref{fig:MAP_rate}.
\begin{theorem}[MAP Upper Bound]
 Introducing $a=n_0 \frac{\mu_0}{\mu^*}$, the expected suboptimality of the MAP of $\cN(0,\mu^*)$ with prior hyper-parameters $(n_0,\mu_0)$ is 
\begin{equation}
	\expect{\bregmanconj( \mu_*; \hat \mu_n^\text{MAP})}
	\leq \begin{cases}
		\bregmanconj( \mu_*; \mu_0) \ \text{ if } \ n=0, \\
		\bregmanconj( \mu_*; \mu_0) + \inv{a} \ \text{if}\ n=1,\\
		\frac{1}{a+n-2} + \frac{\half(1 - \frac{\mu_0}{\mu^*})^2}{(\frac{\mu_0}{\mu^*}+\frac{n-2}{n_0})(1 + \frac{n}{n_0} )} \ \text{if}\  n\geq 2 \ \text{and}\ a=n_0 \frac{\mu_0}{\mu^*} >0
	\end{cases}
	\label{eq:MAP_rate}
\end{equation}
\end{theorem}
Using the same bound on the log~\eqref{eq:log_bound}, we can also get a loose bound on the MLE, which is useful for comparison purposes.
\begin{equation}
	\expect{\bregmanconj( \mu_*; \hat \mu_n^\text{MLE}) }
	\leq \inv{n-2} \; .
	\label{eq:MLE_loose_rate}
\end{equation}

\end{example}

\begin{figure}[ht]
	\centering
	\includegraphics[width=.7\textwidth]{figs/MAP_rates/MAP_rate_n0=5.pdf}
	\caption{
	Convergence rate of MAP~\eqref{eq:MAP_rate} 
	\label{fig:MAP_rate} against the ratio between initialization and optimum $ \frac{\mu_0}{\mu^*}$. 
	We also report the tight convergence rate of MLE~\eqref{eq:MLE_rate} (dash-dotted horizontal lines), and the loose upper bound~\eqref{eq:MLE_loose_rate} (horizontal lines). 
	We report these curves for various values of $n$, and $n_0=5$. 
	We observe that the rate of MAP is significantly better (lower) than the rate of MLE, only when we guessed $\mu_0$ right, within an order of magnitude from $\mu^*$, and for a small number of sample ($n<10$). 
	This observation hints towards the optimality of MLE for large $n$. 
	It makes me feel very interested in convergence rates of MAP in the overparametrized regime $n<\dim(T(X))$.
	}
\end{figure}


\section{Proofs for Gaussian Variance}
\subsection{MLE Tight Upper Bound}
	\begin{theorem}[MLE tight upper bound]
	The MLE of $\cN(0,\mu_*)$ is $\mu_n = \inv{n} \sum_i X_i^2 $.
	Its expected suboptimality is infinite when $n\leq 2$ and otherwise
	\begin{align}
		 \expect{\bregmanconj( \mu_*; \mu_n) }
			\leq \inv{2n} +\frac{2}{n(n-2)} \; .
	\end{align}
	\end{theorem}
	
	\begin{proof}
	The ratio to the optimum  $\frac{\mu_n}{\mu_*}$follows a \href{https://en.wikipedia.org/wiki/Chi-square_distribution}{Chi-square distribution} with $n$ degrees of freedom $\chi^2(n)$ divided by $n$. Its inverse $\frac{\mu_*}{ \mu_n} $ follows an \href{https://en.wikipedia.org/wiki/Inverse-chi-squared_distribution}{inverse Chi-square distribution} with expectation 
	\begin{align}
		\expect{\frac{\mu_*}{ \mu_n}  - 1} 
		= \expect{\frac{n}{\chi^2(n)} - 1} =
		\begin{cases}
			\frac{n}{n-2} -1 = \frac{2}{n-2} \ \text{ if } n>2, \\
			+\infty \  \text{ otherwise. }
		\end{cases}
	\end{align}
	There is also a closed form solution for the expected logarithm of a Chi-squared\citep{pav2015moments}
	\begin{align}
		\expect{\log \frac{\mu_n}{\mu_*}} = \psi(\half[n]) - \log(\half[n])
	\end{align}
	where $\psi$ is the \href{https://en.wikipedia.org/wiki/Digamma_function}{digamma function.}
	Consequently the suboptimality of the MLE has a closed form solution
	\begin{align}
	\expect{\bregmanconj( \mu_*; \mu_n) }
	&= \half \expect{\frac{\mu_*}{ \mu_n} - 1 + \log \left(\frac{\mu_n}{\mu_*} \right) } \\
	& =
	\begin{cases}
		\half \left ( \frac{2}{n-2} +\psi(\half[n]) - \log(\half[n]) \right )  \ \text{ if } n>2, \\
			+\infty \  \text{ otherwise. }
	\end{cases}
	\end{align}
	It is surprising that we need more than $3$ samples for the loss to have a bounded expectation. 
	It is also very pleasant that the optimal value $\mu_*$ does not appear in this rate, so that the convergence rate is independent of the actual solution.
	When the expectation is finite, we can make its formula intelligible thanks to \href{https://en.wikipedia.org/wiki/Digamma_function#Inequalities}{bounds on the digamma function}
	\begin{align}
		-\inv{x} \leq \psi(x) - \log(x) \leq -\inv{2x}	\; .
	\end{align}
	  For $n\geq 3$,
	\begin{align}
		\inv{n-2} - \inv{n}
		&\leq \expect{\bregmanconj( \mu_*; \mu_n) }
		\leq \inv{n-2} - \inv{2 n} \\
		\iff
			\frac{2}{n(n-2)}
			&\leq \expect{\bregmanconj( \mu_*; \mu_n) }
			\leq \inv{2n} +\frac{2}{n(n-2)}
	\end{align}
	so we get a $O(n^{-2})$ lower bound and a $O(n^{-1}) + O(n^{-2})$ upper bound.
	\end{proof}
	Using the \href{https://en.wikipedia.org/wiki/Digamma_function#Recurrence_formula_and_characterization}{Stirling series}, we can also get an asymptotic approximation
	\begin{align}
		\expect{\bregmanconj( \mu_*; \mu_n) }
		\in \inv{2n} + \frac{11}{6 n^2} + \frac{4}{n^2(n-2)} + O(n^{-4}) \; .
	\end{align}
	
	
\subsection{MLE Loose Upper Bound}
If we apply \eqref{eq:log_bound} to the MLE, we get the upper bound
\begin{align}
	\expect{\bregmanconj(\mu^*; \MAPm^\text{MLE})}
	&\leq  \half \left( \expect{\frac{n}{\chi^2(n)}} + \expect{\frac{\chi^2(n)}{n}} - 2 \right ) \\
	&= \half \left( \frac{n}{n-2} + 1 - 2 \right)  
	= \frac{1}{n-2}
\end{align}
which is exactly  an additive term $1/2n$ larger than \eqref{eq:MLE_rate}. This highlights the factor $2$ difference between the log-bound and an exact computation of the expectation of the log term, and a tight upper bound on the digamma function.

\subsection{Expected MAP Natural Parameter}
\begin{lemma}[Expected MAP natural parameter]
	The expectation of the natural parameter of the MAP of $\cN(0,\mu_*)$ is bounded as
	\begin{align}
		\frac{\mu^*}{\mu_n}
		\leq \expect{\frac{\mu^*}{\MAPm}} 
		= \expect{\frac{\MAPt}{\natp^*}} 
		\leq \frac{1}{\frac{\mu_n}{\mu^*} - \frac{2}{n+n_0}} \; .
	\end{align}
\end{lemma}

\begin{proof}
	To start, let us plug in the definition of $\MAPm$
	\begin{align}
		\expect{\frac{\mu^*}{\MAPm}} 
		= \expect{\frac{\MAPt}{\natp^*}} 
		&= \expect{\frac{(n_0+n) \mu^*}{n_0 \mu_0 + \sum_i X_i^2}}  \\
		&= (n_0 + n) \expect{\inv{n_0 \frac{\mu_0}{\mu^*}+ \sum_i \frac{X_i^2}{ \mu^*} }} \\
		&= (n_0 +n) \expect{\inv{a + \chi^2(n)}}
	\end{align}
	where $\chi^2(n) = \sum_i \frac{X_i^2}{ \mu^*}$ is a chi-square random variable of degree $n$ and 
\begin{important}
	\begin{align}
			a=n_0 \frac{\mu_0}{\mu^*} \; .
	\end{align}
\end{important}
	We reformulate this expectation with the trick
	\begin{align}
		&\int_1^\infty e^{-\frac{zt}{2}} dt = \frac{2}{z} e^{-\half[z]} \\
		\implies &\inv{z} = \half e^{\half[z]} \int_1^\infty e^{-\frac{zt}{2}} dt \\
		\implies &\inv{a+x} = \half e^{\frac{a+x}{2}} \int_1^\infty e^{- \frac{(a+x)t}{2}}dt \; ,
	\end{align} 
	to get two integrals instead of one
	\begin{align}
		2^{\half[n]}\Gamma(\half[n]) \expect{\inv{a + \chi^2(n)}}  
		&= \int_0^\infty \frac{x^{\frac{n}{2}-1}e^{-\half[x]} }{a+x} dx \\
		&= \int_0^\infty dx\  x^{\frac{n}{2}-1}e^{-\half[x]} \half e^{\half[a+x]} \int_1^\infty  dt\  e^{-\half[(a+x)t]}  \\
		&= \half e^{\half[a]} \int_1^\infty dt \ e^{-\half[a t]}   \int_0^\infty dx \  x^{\frac{n}{2}-1} e^{-\half[x t]}  \; ,
	\end{align}
	where we used Fubini to switch integrals, without further justification.
	Now with the change of variable $x = 2\frac{y}{t}$, eg $dx = 2\frac{dy}{t}$, the inner integral becomes
	\begin{align}
		\expect{\inv{a + \chi^2(n)}}  
		&= \frac{e^{\half[a]} }{2^{\half[n]+1}\Gamma(\half[n]) }\int_1^\infty dt \ e^{-\half[a t]}   \int_0^\infty 2\frac{dy}{t} \  (2\frac{y}{t})^{\frac{n}{2}-1} e^{-y} \\
		&= \frac{e^{\half[a]} }{2\Gamma(\half[n]) } 
		\underbrace{\int_1^\infty  \frac{e^{-\half[a t]}}{t^{\frac{n}{2}}}  dt}_{E_{\half[n]}(\half[a])  } \ 
		\underbrace{\int_0^\infty  y^{\frac{n}{2}-1} e^{-y} dy}_{\Gamma(\half[n])}
	\end{align}
	so we finally get  the formula valid for all $n\geq 1$
	\begin{align}
		\boxed{\expect{\inv{a + \chi^2(n)}}  = \half e^{\frac{a}{2}} E_{\half[n]}(\half[a])}
		\label{eq:hard_expectation}
	\end{align}
	where the \href{https://dlmf.nist.gov/8.19}{generalized exponential integral function} is defined as 
	\begin{align}
		E_k(z) = \int_1^\infty \frac{e^{-z t} }{t^k} dt \; .
	\end{align}
	Now our goal is to bound this function with simpler functions.
	Fortunately, mathematicians have been working on these integrals for decades.
	When $n=2$, eg $k=1$, we have the simple exponential integral function which \href{https://dlmf.nist.gov/6.8\#E1}{verifies the bound}
	\begin{align}
		\half \log(1 + \frac{2}{x}) 
		\leq e^x E_1(x) 
		\leq \log(1+\inv{x}) \leq \inv{x} \; .
	\end{align}
	For $n \geq 2$, eg $k\geq 1$, we have \href{https://dlmf.nist.gov/8.19\#E21}{the general bound}
	\begin{align}
		\inv{x+k}
		\leq e^x E_k(x) 
		\leq \inv{x + k - 1}
	\end{align}
	so that 
	\begin{align}
		\inv{a+n}
		&\leq \expect{\inv{a + \chi^2(n)}}
		\leq \inv{a + n - 2} \\
		\iff \frac{n +n_0}{a + n}	
		&\leq  \expect{\frac{\mu^*}{\MAPm}} 
		\leq \frac{n +n_0}{a + n - 2}	\\
		\iff \frac{\mu^*}{\mu_n}
		&\leq \expect{\frac{\mu^*}{\MAPm}} 
		\leq \frac{1}{\frac{\mu_n}{\mu^*} - \frac{2}{n + n_0}}  
		\label{eq:theta_expectation_bound}
	\end{align}
	where we used $\frac{a + n}{n +n_0} = \frac{\mu_n}{\mu^*}$.
\end{proof}

We are left with a special case when  $n=1$, eg $k=\half$. Then the integral can be written with the complementary error function, or \href{https://dlmf.nist.gov/7.8#E1}{Mill's ratio} $M(x) = e^{x^2} \int_x^\infty e^{-t^2} dt$. Indeed, taking $t= x \sqrt{u}$
\begin{align}
	\int_x^\infty e^{-t^2} dt = x \int_1^\infty \frac{e^{-x^2 u}}{\sqrt{u}} du = x E_{\half}(x^2) \\
	\implies M(x) = x e^{x^2} E_{\half}(x^2)
\end{align}
so that 
\begin{align}
		\half e^{\half[a]} E_{\half[1]}(\half[a]) 
		= \half \sqrt{\frac{2}{a}} M(\sqrt{\half[a]})
		= \inv{a} \sqrt{\half[a]} M(\sqrt{\half[a]}) \; .
\end{align}
and \href{https://dlmf.nist.gov/7.8#E5}{we known that} $x M(x) < \half$ so we get 
\begin{align}
	\expect{\inv{a + X^2}} < \inv{2 a}
\end{align}
FUCK That's  trivial as  it's true for 1/a.

\subsection{MAP Upper Bound}
\begin{theorem}[MAP Upper Bound]
 The expected suboptimality of the MAP of $\cN(0,\mu^*)$ with prior hyper-parameters $(n_0,\mu_0)$ is 
\begin{equation}
	\expect{\bregmanconj( \mu_*; \hat \mu_n^\text{MAP})}
	\leq \begin{cases}
		\bregmanconj( \mu_*; \mu_0) \ \text{ if } \ n=0, \\
		\bregmanconj( \mu_*; \mu_0) +\inv{a} \ \text{if}\ n=1,\\
		\frac{1}{a+n-2} + \frac{\half(1 - \frac{\mu_0}{\mu^*})^2}{(\frac{\mu_0}{\mu^*}+\frac{n-2}{n_0})(1 + \frac{n}{n_0} )} if n\geq 2 \ \text{and}\ a=n_0 \frac{\mu_0}{\mu^*} >0
	\end{cases}
\end{equation}
\end{theorem}

$\frac{1+n_0}{4} e^{\half[a]} \sqrt{\frac{2 \pi}{a}} \mathrm{erfc}(\sqrt{\half[a]})  - \half  + \frac{a - n_0}{2(1+n_0)} $

\begin{proof}
		Wolfram finds no closed form for $\expect{\log\frac{\MAPm}{\mu^*}}$ so we focus solely on the simpler upper bound given Now using the bound \eqref{eq:log_bound} on the logarithm, we get
\begin{align}
	2\expect{\bregmanconj(\mu^*; \MAPm)} 
	&\leq \expect{\frac{\mu^*}{\MAPm}}  + \expect{\frac{\MAPm}{\mu^*}} - 1 \\
	&\leq \frac{\mu^*}{\mu_n - \frac{2 \mu^*}{n + n_0}}   + \frac{\mu_n}{\mu^*} - 1 \\
	&= \frac{n_0 + n}{a +n - 2} + \frac{a+n}{n_0 + n} - 2 \\
	&= \frac{n_0 - a + 2}{a +n - 2} + \frac{a - n_0}{n_0 + n} \\
	&= \frac{2}{a+n-2} + \frac{(n_0 - a)^2}{(a+n-2)(n_0 + n)} \\
	\implies \expect{\bregmanconj(\mu^*; \MAPm)} 
	&\leq \frac{1}{a+n-2} + \frac{(n_0 - a)^2}{2(a+n-2)(n_0 + n)} \\
	&\leq \frac{1}{a+n-2} + \frac{\half(1 - \frac{\mu_0}{\mu^*})^2}{(\frac{\mu_0}{\mu^*}+\frac{n-2}{n_0})(1 + \frac{n}{n_0} )} 
\end{align}
where recall that $a=n_0\frac{\mu_0}{\mu^*}$. 
This inequality highlights a clear variance-bias decomposition, to compare with the MLE. Unfortunately it is not tight asymptotically as the inequality \eqref{eq:log_bound} is not quadratically tight around $1$. We are basically losing a factor 2 compared to $1/2n$. by~\eqref{eq:log_bound}.
\end{proof}

\tableofcontents

\section{Comparison with  Euclidean Distance}
Often we can relate this Bregman suboptimality with the $\ell^2$ distance in mean parameter space.

\paragraph{If $\conj$ is $L$-Lipschitz} (e.g. $\logpart$ is defined within the $\ell^2$-ball of radius $L$), then
\begin{align}
    \bregmanconj(\mu^* ; \mu) 
    \leq L \norm{\mu^* - \mu} + \norm{\natp} \norm{\mu^* - \mu}
    \leq 2L \norm{\mu^* - \mu}
\end{align}
so $\bregmanconj$ is $2L$-Lipschitz.
Since the empirical average converges in expectation to the population mean at a rate of $1/\sqrt{n}$ in $\ell^2$ norm, we know that this bound applies to the log-likelihood.

\paragraph{If $\conj$ is $L$-smooth} (e.g. $\logpart$ is $L^{-1}$-strongly convex), then
\begin{align}
    \bregmanconj(\mu^* ; \mu) 
    \leq \frac{L}{2} \norm{\mu^* - \mu}^2
\end{align}
so $\bregmanconj$ is upper bounded by a quadratic. In expectation, it should converge at a rate $1/n$.

\paragraph{$\ell^2$-norm Analysis}
Let us make these statements more precise. A bound on the variance becomes a bound on the variance of the average
\begin{align}
	\Var T(X) = \E \norm{T(X) - \mu^*}^2 = \sigma^2 \\
	\implies \E \norm{\mu^* -  \inv{n}  \smallsum_i T(x_i)}^2 = \frac{\sigma^2}{n} 
\end{align}
eg the variance of the mean is $n$ times smaller than the variance of the samples.
Adding a reference mean $\mu_0$ to get the MAP yields
\begin{align}
	\E \norm{\mu^* -  \frac{n_0 \mu_0 + \smallsum_i T(x_i)}{n_0+n} }^2 
	&= \frac{n}{(n+n_0)^2} \sigma^2 +  \frac{n_0^2}{(n+n_0)^2} \norm{\mu^* -  \mu_0}^2 \\
	&= O\left(\frac{\sigma^2}{n} \right) + O\left(\frac{\norm{\mu^* -  \mu_0}^2}{n^2} \right)
\end{align}
so we have a variance term in $O(n^{-1})$ and a bias term decreasing as $O(n^{-2})$. Let's see if we can get similar estimates for arbitrary exponential families !

\section{Asymptotic Behaviour}
In the limit, the MAP estimates reach a rate $O(1/2n)$. This is shown by approximating the Bregman divergence using a second order Taylor expansion
\begin{align}
    \bregmanconj(\mu^* ; \mu) 
    &= \half (\mu - \mu^*)^\top \nabla^2\conj(\mu^*)(\mu - \mu^*)  
    + O(\norm{\mu - \mu^*}^3)\\
    &=\half  \norm{\mu^* - \mu}^2_{\nabla^2\conj(\mu^*)}
    + O(\norm{\mu - \mu^*}^3)
\end{align}
where we introduced the Mahalanobis distance induced by the matrix 
\begin{align}
    \nabla^2\conj(\mu^*) 
    = \nabla^2\logpart(\natp^*)^{-1} 
    = \Cov_{\natp^*}[T(X)]^{-1} 
    =: \mSigma^{-1}  \; .
\end{align}
To exploit this approximation, let us extend the $\ell^2$ norm results to a Mahalanobis distance induced by matrix $\mM$
\begin{align}
	\E \norm{\mu^* -  \inv{n}  \smallsum_i T(x_i)}_\mM^2 = \inv{n} \Tr(\mM \mSigma)
\end{align}
where $\mSigma$ is the covariance of $T(X)$. We retrieve $\nicefrac{\sigma^2}{n}$, the variance divided by the number of samples, when the metric is the identity $\mM=\mI$.  For the MLE we get
\begin{align}
	\E \bregmanconj \left (\E [T(X)] ;  \inv{n}  \smallsum_i T(X_i) \right ) = \frac{d}{2n} + O(n^{- \frac{3}{2}})
\end{align}
and for the MAP we  get 
\begin{align}
	\E \norm{\mu^* -  \frac{n_0 \mu_0 + \smallsum_i T(x_i)}{n_0+n} }^2_{\mSigma^{-1}}
	&= \frac{n d}{(n+n_0)^2}  +  \frac{n_0^2}{(n+n_0)^2} \norm{\mu^* -  \mu_0}^2_{\mSigma^{-1}} \\
	&= \frac{d}{n} + O\left(\frac{1 + \norm{\mu^* -  \mu_0}^2_{\mSigma^{-1}} }{n^2} \right) \; .
\end{align}
or 
\begin{align}
\label{eq:MAP_asymptote}
	\E \bregmanconj \left (\E [T(X)] ;  \frac{n_0 \mu_0 + \smallsum_i T(x_i)}{n_0+n} \right ) 
	= \frac{d}{2n} + O(n^{- \frac{3}{2}})
\end{align}
Remark how the variance does not even appear, only the dimension divided by $n$ really matters. I find this result quite spectacular : the convergence speed of MLE is not affected by the covariance of sufficient statistics, and for MAP it matters only in the $O(n^{-2})$ term.
Indeed this is because it is hidden within the Bregman divergence loss. 
Anyway that's all good for asymptotic results, but we are interested in a finite sample analysis. How do we get this ?

\section{Bias-Variance Decomposition}
\paragraph{Covariance.}
Using Jensen inequality and inserting appropriate quantities, we obtain the following analog of the bias-variance decomposition:
\newcommand{\bias}[1]{\textcolor{Red}{\underbrace{\textcolor{black}{#1}}_{\text{bias}}}}
\newcommand{\variance}[1]{\textcolor{Green}{\underbrace{\textcolor{black}{#1}}_{\text{variance}}}}
\begin{align}
	& \expect{\bregmanconj(\mu^* ; \hat \mu_n)} 
	= \conj(\mu^* ) 
	- \underbrace{\expect{\conj(\hat \mu_n)}}_{\leq \conj(\mu_n) \text{ (Jensen)} } 
	\pm \langle \underbrace{\nabla \conj( \mu_n)}_{=:\natp_n} ; \mu^* - \mu_n \rangle
	+ \E[\langle \hat \natp_n ; \hat \mu_n   \pm \mu_n - \mu^*  \rangle ] \nonumber \\
	& \leq \conj(\mu^* ) 
	- \conj(\mu_n) 
	- \langle  \nabla \conj( \mu_n) ; \mu^* - \mu_n \rangle
	+ \langle \E[\hat \natp_n] - \natp_n ;  \mu_n - \mu^* \rangle
	+ \E[\langle \hat \natp_n ; \hat \mu_n - \mu_n \rangle ]
	\nonumber \\
	& \leq \bias{\bregmanconj(\mu^*; \mu_n)}
	+ \frac{n_0}{n+n_0} \langle  \bias{\mu_0 - \mu_*} ; \variance{\E[\hat \natp_n] - \natp_n} \rangle 
	+ \variance{\Cov(\hat \natp_n ; \hat \mu_n ) }
\end{align}
where $\hat \mu_n = \frac{n_0 \mu_0 + \sum_i T(X_i)}{n_0 + n}$ is the MAP estimate of the mean parameter, $\hat \natp_n = \nabla \conj(\hat \mu_n)$ is the corresponding natural parameter, 
$\mu_n = \expect{\hat \mu_n} = \frac{n_0 \mu_0 + n \mu^*}{n_0 + n}$ is the expected MAP estimate,
and $\theta_n = \nabla \conj(\mu_n)$ is the natural parameter of this expectation.
Note that unless $\conj$ is $\ell^2$, or we are in a degenerate case with $\hat \mu_n$ a Dirac, $\theta_n \neq \expect{\hat \theta_n}$.

\begin{figure}[ht]
	\centering
	\includegraphics[width=.8\textwidth]{figs/bias-variance.png}
	\caption{A schematic illustration of the different characters featured in the bias-variance decomposition.}
	\label{fig:bias-variance}
\end{figure}

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{figs/numerical_schema.pdf}
	\caption{A numerical illustration of the different characters featured in the bias-variance decomposition, for a 1D Gaussian $\cN(\mu, \sigma^2)$.}
	\label{fig:bias-variance-numerical}
\end{figure}

\paragraph{No Jensen.}
In fact this previous decomposition is highly suboptimal. 
We do not need to use Jensen inequality.
\begin{align}
	\expect{\bregmanconj(\mu^* ; \hat \mu_n)} 
	&= \conj(\mu^* ) 
	\pm \conj(\mu_n)
	-  \expect{\conj(\hat \mu_n)}
	+ \E[\langle \hat \natp_n ; \hat \mu_n   \pm \mu_n - \mu^*  \rangle ] \\
	&= \conj(\mu^* ) 
	- \conj(\mu_n)  + \E[\bregmanconj(\mu_n, \MAPm)]
	+ \langle \E[\hat \natp_n] \pm \nabla \conj(\mu_n) ;  \mu_n - \mu^* \rangle
	\nonumber\\
	& = \bias{\bregmanconj(\mu^*; \mu_n)}
	+ \frac{n_0}{n+n_0} \langle  \bias{\mu_0 - \mu_*} ; \variance{\E[\hat \natp_n] - \natp_n} \rangle 
	+ \variance{\E[\bregmanconj(\mu_n, \MAPm)]}
\end{align}
With Jensen, we were losing a Bregman term, as per the 3 point property, the covariance is equal to the symmetrized Bregman.
\begin{align}
	\Cov(\hat \natp_n ; \hat \mu_n )  
	=\expect{\bregmanconj(\MAPm ; \mu_n)
	+ \bregmanconj(\mu_n ; \MAPm)}
\end{align}

\paragraph{No Mixed Term.}
In fact, it is possible to have no mixed term using the central point $\tilde \mu_n = \nabla \logpart(\E[\MAPt])$ instead of $\mu_n = \E[\MAPm]$,  exactly as described by \citet[Theorem 0.1]{pfau2013generalized}.
\begin{align}
	\expect{\bregmanconj(\mu^* ; \hat \mu_n)} 
	= \bias{\bregmanconj(\mu^* ; \tilde \mu_n)}
	+ \variance{\expect{\bregmanconj(\tilde \mu_n ; \MAPm)}}
\end{align}
Now the big question, is : what kind of results do I get with such quantities ? In general, it seems that $\tilde \mu_n$ is a very central and interesting quantity to consider.  We show these decompositions in Figure~\ref{fig:experimental_decomposition}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{{figs/biasvariance/ratio=0.8_n0=10}.pdf}
	\caption{
	\textbf{Left:} training curves and analytic upper bound. 
	\textbf{Center:} bias-mixed-variance decomposition, using the arithmetic mean.
	\textbf{Right:} bias-variance decomposition, using the harmonic mean.
	}
	\label{fig:experimental_decomposition}
\end{figure}

\subsection{Pure Bias Term}
The pure bias term measures the divergence between the optimum and the expected MAP estimate $\bregmanconj(\mu^*; \mu_n)$.

\paragraph{Aymptote.}
Using the quadratic approximation of the Bregman divergence near $\mu^*$, we get a $O(n^{-2})$ rate for the pure bias term
\begin{align}
	\bregmanconj(\mu^*; \mu_n) 
	= \frac{\norm{\mu^* -  \mu_0}^2_{\mSigma^{-1}}}{2(1 + \frac{n}{n_0})^{-2} } + O(n^{-3}) \; .
\end{align}

\begin{example}
We can use the classical lower bound on the logarithm to upper bound the pure bias term 
	\begin{align}
		\bregmanconj(\mu^*; \mu_n)
		 &= \frac{\mu^*}{\mu_n} -1  - \log \frac{\mu^*}{\mu_n}  \\
		 &\leq \frac{(\mu^*- \mu_n)^2}{2 \mu^* \mu_n} 
		 = \half ( \frac{\mu^*}{\mu_n} + \frac{\mu_n}{\mu^*	}) - 1 \\		 
		 & =  \half (\frac{n_0 + n}{a + n} -1 + \frac{a +n}{n_0+n} -1) \\
		 & =  \half (\frac{n_0 - a}{a+n} + \frac{a - n_0}{n_0 + n}) \\
		 & = \half[n_0-a] (\inv{a+n} - \inv{n_0+n}) \\
		 &= \half[n_0-a] \frac{n_0 + n - a- n}{(a+n)(n_0+n)} \\
		 &= \frac{(n_0-a)^2}{2(a+n)(n_0+n)}
	\end{align}
	which is a $O(n^{-2})$ term, worth $0$ when $n_0=a$, eg $\mu_0=\mu^*$, exactly as we expected.
\end{example}

\subsection{Pure Variance Term}
The pure variance term $\Cov(\hat \natp_n ; \hat \mu_n )$   can be seen as a non-linear variance of $\hat \mu_n$.
Indeed, it is positive thanks to the monotonicity of $\nabla \conj$
\begin{align}
&\langle \hat \natp_n - \natp_n ; \hat \mu_n - \mu_n \rangle 
	= \langle \nabla \conj (\hat \mu_n) - \nabla \conj(\mu_n) ;  \hat \mu_n - \mu_n \rangle 
	\geq 0
\end{align}
Taking the expectation, (which allows removing the constant $\theta_n$) 
\begin{align}
	\expect{\langle \hat \natp_n - \natp_n ; \hat \mu_n - \mu_n \rangle} 
	= \Cov(\hat \natp_n ; \hat \mu_n ) \geq 0
\end{align}
Consequently, we will now write about this covariance as a variance, with the monotone operator $\nabla \conj$ (or equivalently $\nabla \logpart$) plugged in the middle
\begin{align}
	\Var_{\nabla \conj}(\hat \mu_n) := \Cov(\nabla\conj(\hat \mu_n) ; \hat \mu_n ) \geq 0 \; .
\end{align}

\paragraph{Expansion}
We can expand  $\hat \mu_n - \mu_n$ to get another formulation of this variance
\begin{align}
	\expect{\langle \hat \natp_n ; \hat \mu_n - \mu_n \rangle}
	&= \expect{\langle \hat \natp_n ; \frac{\sum_i T(X_i) - \mu^*}{n+n_0} \rangle}  \\
	& = \frac{n}{n+n_0} \expect{\langle \hat \natp_n ; T(X_0) - \mu^*\rangle}
	= \frac{n}{n+n_0} \Cov(\hat \natp_n ; T(X_0) ) 
\end{align}
where we used the property that all data points play a symmetric role to remove the sum over $i$.


\paragraph{Asymptote.}
Using a linear approximation of the mirror map, we get a $O(n^{-1})$ rate for the variance term
\begin{align}
	&\nabla \conj(\mu)  
	= \nabla \conj(\mu^*) + \nabla^2\conj(\mu^*)(\mu - \mu^*) + O(\norm{\mu - \mu^*}^2) \\
	\implies &\hat \natp_n 
	= \natp^* + \Sigma^{-1} (\hat \mu_n - \mu^*)  + O(\E[\norm{\hat \mu_n - \mu^*}^2]) \\
	\implies &\Cov(\hat \natp_n ; T(X) ) 
	= \inv{n+n_0} \E[ \norm{T(X) - \mu^*}^2_{\Sigma^{-1}}]   + O(n^{-2}) \\
	\implies &\Cov(\hat \natp_n ; T(X) ) 
	 = \frac{d}{n+n_0} + O(n^{-2}) \; .
\end{align}
Note that we lost a factor $\half$ compared to the straightforward asymptotic analysis~\eqref{eq:MAP_asymptote}, probably because of the Jensen upper bound.

\begin{example}
	For the gaussian variance MAP, the variance term is
		\begin{align}
			\Cov(\hat \natp_n ; \hat \mu_n )  
			= \E[\MAPt \MAPm] - \E[\MAPt] \E[\MAPm] 
			=  - \half - \E[\MAPt] \mu_n
	\end{align}
	where we used the fact that $\natp \mu = -\half$ for all valid pairs $(\natp, \mu)$. 
	Plugging-in \eqref{eq:theta_expectation_bound} yield
	\begin{align}
		-\half + \frac{\mu_n}{2 \mu_n}
		&\leq \Cov(\hat \natp_n ; \hat \mu_n ) 
		\leq -\half + \half \frac{a + n}{a + n - 2} \\
		\iff 0&\leq \Cov(\hat \natp_n ; \hat \mu_n ) 
		\leq \half \frac{a+n - (a + n -2)}{a + n - 2} \\
		\iff 0&\leq \Cov(\hat \natp_n ; \hat \mu_n ) 
		\leq \inv{a +n - 2}
	\end{align}
	which is exactly the kind of upper bounds we were looking for !
\end{example}

\subsection{Mixed Bias-Variance Term}
The mixed bias variance term is $ \frac{n_0}{n+n_0} \langle  {\mu_* - \mu_0} ; {\natp_n - \E[\hat \natp_n]} \rangle $.
Note that it might be negative !
Let's focus on the right factor
\begin{align}
\theta_n - \expect{\hat \theta_n} = \nabla \conj(\E[\hat \mu_n]) - \E[\nabla \conj(\hat \mu_n)]
\end{align}
is sort of a commuting bracket between $\nabla \conj$ and the expectation over $\hat \mu_n$. It will be large if $\nabla \conj$ is highly non linear over the distribution of $\hat \mu_n$.

\paragraph{Asymptote.}
A linear approximation of $\nabla \conj$ shows that the mixed term is in $O(n^{-3})$, since the linear function commutes with the expectation.
We need to use a quadratic approximation of $\nabla \conj$ to get a non-trivial approximation
\begin{align}
	&\nabla \conj(\mu)  
	= \nabla \conj(\mu^*) + \nabla^2\conj(\mu^*)(\mu - \mu^*) +  \half \nabla^3 \conj(\mu^*) [ (\mu - \mu^*)  ; (\mu - \mu^*) ]
	+ O(\norm{\mu - \mu^*}^3) \\
	&\implies 
	\nabla \conj(\E[\hat \mu_n]) - \E[\nabla \conj(\hat \mu_n)]
	= \half \nabla^3 \conj (\mu^*) \Cov(\hat \mu_n) 
	+ O(\E[\norm{\hat \mu_n - \mu^*}^3]) \\
	&\implies 
	\nabla \conj(\E[\hat \mu_n]) - \E[\nabla \conj(\hat \mu_n)]
	= \frac{\nabla^3 \conj (\mu^*) \Sigma}{2(n+n_0)^2} 
	+ O(n^{-3}) \\
\end{align}
where the axis of the tensor product between third derivative and vector / matrix are all kept implicit for simplicity.
Consequently the bias-variance term is 
\begin{align}
	\frac{n_0}{n+n_0} \langle  \bias{\mu^* - \mu_0} ; \variance{\natp_n - \E[\hat \natp_n]} \rangle
	 = \frac{n_0}{(n+n_0)^3} (\mu^* - \mu_0)^\top \nabla^3 \conj (\mu^*) \Sigma + O(n^{-4}) = O(n^{-3}) \; .
\end{align}

\begin{example}
	As we have the tight bounds \eqref{eq:theta_expectation_bound} for the expectation of the natural parameter of the MAP, we can bound above and below this mixed term
	\begin{align}
		0
		\leq \natp_n - \E[\MAPt]
		\leq \frac{n_0 +n}{2 \mu^*} \left ( \inv{a+n-2} - \inv{a+n} \right)
		 = \frac{n_0+n}{\mu^*(a+n)(a+n-2)}	 \in O(n^{-1}) \; .
	\end{align} 
	Remark how this asymptotic class differs from the $O(n^{-2})$ I expected. Is this bound less tight than I thought, or are my calculus wrong ? 
	Also remark that $\natp_n$ is always greater than $\E[\MAPt]$, which is a good sanity-check because $\nabla \conj(\mu) = - \inv{2\mu}$ is a concave function.

	The rate on the mixed term depends on the sign of $\mu^* - \mu_0 $
	\begin{align}
		\frac{n_0}{n+n_0} \langle  \mu^* - \mu_0 ; \natp_n - \E[\hat \natp_n] \rangle
		\leq \begin{cases}
			0 \ \text{ if } \ \mu^*\leq \mu_0, \\
			\frac{n_0 - a}{(a+n)(a+n-2)} \ \text{ otherwise.}
		\end{cases}
	\end{align}
\end{example}


\subsection{Putting it all together}
\begin{example}
\begin{align}
	\expect{\bregmanconj(\mu^* ; \hat \mu_n)} 
	\leq \bias{\frac{(n_0-a)^2}{2(a+n)(n_0+n)}}
	+ \frac{\max(n_0 - a , 0)}{(a+n)(a+n-2)}
	+ \variance{\frac{1}{a+n-2}}
\end{align}
\end{example}


\section{Optimization Perspective}
\paragraph{Problem.}
Our goal is to solve the following problem
\begin{align}
	\min_\natp \E[f(\natp ; X)] := -\langle \E[T(X)] ; \natp \rangle + \logpart(\natp)
\end{align}
where $\logpart$ is a convex function and we only have access to stochastic oracles $X \sim p(X | \natp^*)$, meaning we can compute stochastic values and gradients 
\begin{align}
	f(\natp ; X) &= -\langle T(X) ; \natp \rangle + \logpart(\natp) \\
	\nabla f(\natp ; X) &= \nabla \logpart(\natp) - T(X)
\end{align}
such that $\E[T(X)] = \mu^*$.

\paragraph{Algorithms.}
We observe $n$ data points $X_i$. 
We are studying two algorithms to solve this problem : the MLE and the MAP
\begin{align}
	\MAPm^\text{MLE} &= \inv{n}\sum_{i=1}^n T(X_i) 
	&\MAPt^\text{MLE} &= \nabla\logpart^{-1}(\MAPm^\text{MLE}) \\
	\MAPm^\text{MAP} &= \frac{n_0\mu_0 + \smallsum_{i=1}^n T(X_i)}{n_0 + n}
	&\MAPt^\text{MAP} &= \nabla\logpart^{-1}(\MAPm^\text{MAP})	
\end{align}

\paragraph{Interpretation.}
As we will see in the two upcoming sections, MAP can be seen as an application of more generic iterative algorithms. 
\begin{itemize}
	\item stochastic bregman proximal point with divergence $\bregman$, initialization $\natp_0$, and learning rate $\inv{n_0+n}$,
	\item stochastic mirror descent with divergence $\bregman$, initialization $\natp_0$, and learning rate  $\inv{n_0+n +1}$.
\end{itemize}
In both cases, $f(\natp)$ is convex and 1-smooth / 1-strongly convex relative to the potential $\logpart$. Be cautious that the learning rate is decreasing.


\paragraph{Other assumptions.}
An assumption pointed out by Nicolas Loizou is 
\begin{align}
	f(\natp^*) - \E_{X\sim\natp^*}[\min_\natp f(\natp ; X)] < \infty
\end{align}
for which a sufficient condition is 
\begin{align}
\forall X, \min_\natp f(\natp ; X) >	 - \infty \iff \max_\natp p(X|\natp) < \infty
\end{align}
which holds in a lot of settings but not all. For instance, a gaussian $\cN(\mu,\sigma^2)$ can overfit on a single data point and give it infinite density -- eg converge to a Dirac. However we can overcome this issue by grouping data points : a gaussian can not overfit on two points. Aggregating points is simple : average sufficient statistics, exactly like in MLE or MAP $T'(X_1,X_2) = \frac{T(X_1) + T(X_2)}{2}$. 
So in general we may say that this hypothesis holds, at the expense of dividing the number of samples by some number $n/k$. What kind of results can it give us my dear Nicolas Loizou ? ;)



\section{Stochastic Bregman Proximal Point}
Remark that the MAP estimate minimizes the sum of a stochastic loss $ -\log p(X|\natp)$ and a deterministic divergence to an initial point $n_0 \bregman(\natp ; \natp_0)$. This is a stochastic Bregman proximal step with step-size $\inv{n_0}$. This can also be seen at each step since
\begin{align}
    \hat \natp_{n+1} 
    &= \argmin_\natp -\log p(x_{n}|\natp) + (n_0 + n) \bregman(\natp ; \hat \natp_n) \\
    &= \argmin_\natp f(\natp; x_{n}) +  \inv{\gamma_{n}}\bregman(\natp ; \hat \natp_n) \; .
\end{align}
Hence the MAP estimate can also be seen as the result of a stochastic proximal Bregman point algorithm with step-size $\gamma_n = \inv{n_0 + n}$ at step $n$.

This is similar to the online learning setup, and it may be possible to bound the regret, with approaches similar to Adagrad.

The analysis of deterministic Bregman proximal point relies on the three points lemma (Appendix~\ref{app:3points}). First one can prove descent, then one can prove that the total path length and the sum of suboptimalities are bounded, but this does not transpose immediately to the stochastic setting. One needs an assumption on the quality of the stochastic estimates $f(. ; x)$. 
Given the proof, the most straightforward such assumption (see paper notes) is 
\begin{align}
\expect{f(\natp_{n+1}) - f(\natp_{n+1} ; x_{n})} \leq \gamma_n \sigma^2
\end{align}
which translates to $\Cov(T(x_{n}) , \natp_{n+1}) \leq  \gamma_n \sigma^2$ in our case. This is exactly the same assumption as given by the SMD analysis. However even with this assumption, the convergence rate applies to the iterate $\sum_t \lr_t \natp_{t+1} / \sum_t \lr_t$, which gives more weight to the first iterates. It does not immediately apply to the last iterate.


\section{Stochastic Mirror Descent (SMD)}
\subsection{MAP as SMD}
Let $f(\natp) := \expect[x]{-\log p(x|\natp)} = - \langle \mu_*, \natp\rangle + \logpart(\natp)$. In words, $f$ is linear modification of a convex function $\logpart$, which we can access only through noisy estimates of $\mu_*$. It turns out that the MAP estimate can also be seen as the iterates of stochastic mirror descent with mirror map $\logpart$. First let's recall mirror descent iteration
\begin{align}
	\natp_{t+1} 
	&:= \argmin_\natp \lr \linear_f(\natp; \natp_t) + \bregman(\natp ; \natp_t)  \\
	& = \nabla \conj (\nabla \logpart(\natp_t) - \lr \nabla f(\natp_t))
\end{align}
where $\linear_f(\natp; \natp_t) = f(\natp_t) + \langle \nabla f(\natp_t), \natp - \natp_t \rangle$ is the linear approximation of $f$ in $\natp_t$ evaluated at $\natp$. Solving this problem require solving problems of the form $\argmin_\natp - \langle c, \natp \rangle + \logpart(\natp)$, eg computing the convex conjugate of $\logpart$. 
Note that finding $\natp_*$ is done with 1 step of mirror descent. Indeed plugging in definitions of $f$ and $\mu$ yields
\begin{align}
	&\mu_{t+1}  = \mu_t - \lr (\mu_t - \mu_*) \\
	\implies &\mu_t  = \mu_* + (1- \lr)^t (\mu_0 - \mu_*)
\end{align}
which shows exponential convergence and 1-step convergence when $\lr =1$. Back to our sheep, the MAP iteration can be cast as stochastic mirror descent (SMD),  with $g_t = \nabla f(\natp_t, x_{t+1})$ a stochastic estimate  of $\nabla f(\natp_t)$
\begin{align}
    \hat \natp_{n+1} 
    &= \argmin_\natp 
    - \langle T(x_{t+1}), \natp \rangle  + \logpart(\natp) + (n_0 + n) \bregman(\natp ; \natp_n) \\
    %&(n_0+n)\left(\logpart(\natp) - \logpart(\natp_n) - \langle \nabla \logpart(\natp_n)  , \natp - \natp_n \rangle \right)  \\
    &= \argmin_\natp - \langle T(x_{t+1}), \natp \rangle   + \logpart(\natp_n) + \langle \nabla \logpart(\natp_n)  , \natp - \natp_n \rangle + (n_0+n + 1) \bregman(\natp ; \natp_n) \\
    &= \argmin_\natp \linear_f(\natp;\natp_n, x_{t+1}) + (n_0 + n + 1) \bregman(\natp ; \natp_n)
\end{align}
where $\linear_f(\natp;\natp_n, x_{t+1})$ is the stochastic linearization of $f$ at $\natp_n$ evaluated at $\natp$ with randomness coming from $x_{t+1}$. This is the formula for stochastic mirror descent (SMD) applied to $f$ with mirror map $\logpart$ and step-size $\gamma_n = \inv{n_0 + n + 1}$.

\subsection{Relative Smoothness for Mirror Descent}
In the classic setting, SMD is studied under strong-convexity assumption on the mirror map $\logpart$ \citep{bubeck2015convex}.
In our setting this is not always true -- eg gaussians.
However a recent and fast-expanding body of work is concerned with a new assumption: relative smoothness and relative strong-convexity.
\begin{align}
	& \text{$f$ is $L$-smooth relative to $h$} \\
	\iff & Lh - f \ \text{convex} \\
	\iff & f(y) \leq 	f(x) + \langle \nabla f(x) , y-x \rangle + L\cB_h(y;x), \forall x,y \\
	\iff & \cB_f(y;x) \leq L \cB_h(y;x), \forall x,y \\
	\iff & \nabla^2 f(x) \leq L \nabla^2 h(x),  \forall x
\end{align} 
where the last equivalence holds only when $f$ and $h$ are twice differentiable. In words, $f$ is upper bounded by it linear approximation plus the $h$-Bregman divergence, which can also be seen as a bound between divergences, or more locally as a bound between Hessians. Similarly, $f$ is $\mu$ strongly-convex relative to $h$ if
\begin{align}
	& f-\mu h \ \text{ convex} \\
	\iff & f(x) + \langle \nabla f(x) , y-x \rangle + \mu \cB_h(y;x) \leq f(y)\forall x,y \\
	\iff & \mu\cB_h(y;x) \leq \cB_f(y;x) \forall x,y \\
	\iff & \mu \nabla^2 h(x) \leq \nabla^2 f(x)  \forall x \; .
\end{align} 

Another way to view this elegant generalization of smoothness and strong-convexity is as a transfer of the Loewner partial order on symmetric matrices to functions, via the Hessian. 
As such it can be applied to many functions that were out of reach for $\ell^2$ norm, by taking the appropriate reference function.  For instance $h(x) = -\log(x)$ or $h(x) = x^4$. 
As early as 2011, \citet{birnbaum2011distributed} showed $O(\inv{t})$ convergence rate for mirror descent under smoothness assumption relative to the mirror map. 
More precisely, he proved that when $f$ is $L$-smooth relative to $h$, then the suboptimality of the sequence 
\begin{align}
	x_{t+1}  = \argmin_x \langle \nabla f(x_t), x \rangle + \cB_h(x ; x_t)
\end{align}
is upper bounded by the simple formula
\begin{align}
	\implies f(x_t) - f(x_*) \leq \frac{L \cB_h(x_* ; x_0) }{t} \; .
\end{align}
These notions were rediscovered and expanded by \citet{bauschke2017descent} and \citet{lu2018relatively}. If you need to read one, pick \citet{lu2018relatively} -- I found it much much easier and more enjoyable to read.  This latter paper also derived a linear convergence rate for mirror descent under relative smoothness and strong-convexity, with the relative condition number $\frac{L}{\mu}$ appearing.

\subsection{Relative smoothness for SMD}
Now our setting is Stochastic Mirror Descent (SMD), meaning at each step we observe a random unbiased estimate gradient. This setting was studied by \citet{hanzely2018fastest}, who proved in the smooth strongly-convex case with tail averaging :  with constant step-size, linear convergence down to a variance ball, and with step-size $\gamma_t = n_0 + t$ a rate $\tilde O(\inv{t})$. These results match the rates for standard SGD.

This is very interesting to us.
Let us have a look at their variance assumption \citep[Assumption 5.1]{hanzely2018fastest}.
Let $g_t$ be the random gradient at step $t$  (coming from data point $x_t$), and $\natp_{t+1}$ the next iterate. Then the variance bound $\sigma^2$ is an upper bound on the covariance between the gradient update  $-g_t$ and the descent direction $\natp_{t+1} - \natp_t = \nabla \logpart^*(\nabla \logpart(\natp_t) - \lr_t g_t ) - \natp_t $ that should hold for all time steps  
\begin{align}
	&\Cov(- g_t,\natp_{t+1} - \natp_t | X_{1 \dots t}) \leq \lr_t \sigma^2 \\
	&= \expect{\langle - g_t, \natp_{t+1} \rangle}  - \langle \E[-g_t], \E[\natp_{t+1}] \rangle   \\
	 &= \expect{\langle \nabla f(\natp_t) - g_t , \natp_{t+1}  \rangle}
\end{align}
where $\gamma_t$ is the step-size and expectations are conditional on the past. Remark that when we plug in $\conj = \|.\|^2$, we recover the gradient variance typical of SGD. 

\subsection{Transferring SMD's covariance assumption to MAP estimates}
In our setting, 
\begin{align}
	\nabla f(\natp_t) - g_t =  T(X_{t+1})	- \E[T(X)]
\end{align}
so that $\sigma$ is really a bound on the covariance of the sufficient statistics with another variable
\begin{align}
	&\Cov( T(X_{t+1}), \natp_{t+1}| X_{1\dots t})
	\leq \lr_t\sigma^2 \\
	=& \Cov(T(X_{t+1}), \nabla\conj \left(\lr_t T(X_{t+1}) + (1-\lr_t ) \mu_t \right) | X_{1\dots t}) \;.
\end{align}
In other words, we need for all $\lr\in(0,\inv{n})$, and a certain subset of $\mu$,
\begin{align}
\label{eq:hypvariance}
	\boxed{
	\Cov_X \bigg(T(X), \nabla \conj (\lr T(X) + (1-\lr) \mu)\bigg	) 
	\leq \lr \sigma^2 \; .
	}
\end{align}

\begin{example}
Unfortunately this bound is not trivially satisfied, even for simple exponential family members, such as centered gaussians $\cN(0,\sigma^2)$, when we are aiming to estimate the variance.
The covariance  \eqref{eq:hypvariance} we want to bound is equal to
\begin{align}
	\half \expect{\frac{\sigma_*^2 - X^2 }{ X^2 \lr + (1-\lr) \sigma_0^2 }}
	\approx \lr (1-\lr)^{-2} \left (\frac{\sigma_*}{\sigma_0} \right)^4
\end{align}
using a first order approximation of the inverse, eg a first order approximation of $\nabla \conj$ that holds when $\lr X^2 \ll (1-\lr) \sigma_0^2$. This does not hold for $\sigma_0\ll 1$, which is precisely when the covariance  explodes, as illustrated in Figure \TODO{}. This is a problem, because we need this bound to hold  for all values of $\sigma_0$ in the support of $X^2$, meaning on the whole $[0,+\infty[$, which is impossible because $1/X^2$ diverges, so we need to process this integral differently.   
\end{example}

\paragraph{MLE Analysis.}
When we plug this bound into the expected suboptimality formula~\eqref{eq:bregmanMLE} with the maximum likelihood estimate $\hat \mu =  \inv{n}\sum_i T(X_i)$, we get
\begin{multline}
	\expect[X_{1\dots n}]{\bregmanconj ( \mu_* ;\hat \mu) }
	=\conj(\mu_*) \overbrace{- \E[\conj(\hat \mu)]}^{\leq - \conj(\E[\hat \mu])} \\
	+ \inv{n}\sum_i \E[\underbrace{\E[\langle  T(X_i) - \mu_*; \nabla\conj (\hat \mu) \rangle | X_j, j\neq i]}_{\leq \nicefrac{\sigma^2}{n} \eqref{eq:hypvariance}
	 	\; \text{with}\; \mu=\inv{n-1}\smallsum_{j\neq i} T(x_j)  } ]  \leq \frac{\sigma^2}{n}
\label{eq:MLE_bound}
\end{multline}
where we used the decomposition $\hat \mu = \inv{n}T(X_i) + (1-\inv{n}) \inv{n-1} \sum_{j\neq i}T(X_j)$ to apply~\eqref{eq:hypvariance}. 
In words, this variance assumption on $T(X)$ and $\conj$ immediately gives us a bound on the suboptimality. 

This is too simple to be true, as shown on our trailing example.
In particular we show that it cannot hold for all possible values of $\mu$ within the double expectation in~\eqref{eq:MLE_bound}.
The mean parameter $\mu$ take values arbitrarily close from $0$, which makes the covariance explode.
\RLP{should we get a functional bound $\sigma^2(\mu)$ instead ? or split between $\mu$ smaller or larger than $\varepsilon$.}

\paragraph{MAP Analysis.}
We can also apply this result to the MAP estimate $\hat \mu = \frac{n_0 \mu_0 + \smallsum_i T(X_i)}{n_0+n}$ but I did not manage to reach a satisfying conclusion about the bias. 
Note $\tilde \mu = \E[\hat \mu] = \frac{n_0 \mu_0 + n \mu_*}{n_0+n}$ and $\gamma_0= \frac{n_0}{n_0+n}$ and the step-size $\gamma = \inv{n_0 + n}$ .
\begin{align}
	\expect[X_{1\dots n}]{\bregmanconj ( \mu_* ;\hat \mu) } 
	&\leq \conj(\mu_*) - \conj(\tilde \mu) + \gamma_0 \langle \mu_0 - \mu_* ; \E[\hat \natp] \rangle + (1 - \gamma_0) \gamma \sigma^2  \\
	& = \bregmanconj (\mu_* ; \tilde \mu) - \langle \tilde \mu - \mu_* ;    \tilde \natp - \E[\hat \natp] \rangle + \frac{n}{(n_0+n)^2}\sigma^2
\end{align}
so we recover the $O(n^{-1})$ for the variance and we are still looking for $O(n^{-2})$ rate for the bias. The Bregman term in the bias should be in this spirit, given a quadratic approximation
\begin{align}
	 \bregmanconj (\mu_* ; \tilde \mu)  
	 = \bregmanconj (\mu_* ; \mu_* + \gamma_0 (\mu_0 - \mu_*) ) \approx \half \gamma_0^2 \norm{\mu_0 - \mu_*}^2_{\mSigma_*^{-1}}
\end{align}
where $\mSigma_* = \Cov_{\mu_*}(T(X))$. 
The order of terms $\bregmanconj (\mu_* ; \tilde \mu)  \neq \bregmanconj (\tilde \mu ; \mu_* ) $ only affects third order terms.
That's for the bregman, but the scalar product is harder to bound. Hopefully it could be positive. If not we know that $\tilde \mu - \mu_* = \frac{n_0}{n_0+n} (\mu_0 - \mu_*) = O(n^{-1})$ , and we can hope that the same holds for $\tilde \natp - \E[\hat \natp] = \nabla \conj(\E[\hat \mu]) - \E[\nabla\conj (\hat \mu)]$, which kinda measures the non-linearity, or the curvature of $\conj$. Using a simple cauchy-schwartz, and the hessian of the conjugate, we might be able to get something.


\paragraph{Relative Continuity}
\citet{lu2019relative} define relative continuity to tackle non-differentiable functions -- eg Hinge loss. These functions are typically assumed to be Lipschitz, but that is not always true -- eg squared Hinge loss. Relative continuity generalizes Lipschitz-ness by upper bounding the gradient norm (or expected norm of stochastic gradients) with the ratio between a Bregman and $\ell^2$ distance. Although this hypothesis is far from intuitive, it does yield convergence rates very similar to typical SGD analysis. In this work we are first concerned with strictly convex differentiable functions $\logpart(\natp)$, to ensure a clean bijection between natural and mean parameters.

\paragraph{Bound on the trajectory.}
Bound \eqref{eq:hypvariance} might not hold in general, but it can along the trajectory. Can we prove it using an inductive method such as \citet[Appendix A]{lacoste2012simpler}?
No we cannot because $\mu$ takes every possible values as a random variable within the expectation.

\section{Expected Relative Smoothness}
To avoid gradient variance assumption, we can take inspiration from expected smoothness, which is a consequence of convexity and smoothness, and which yields a general analysis of SGD. Can we get something similar with relativity and SMD? 
The analog of gradient norm is the Bregman between iterates, similar to the Homeomorphic EM rate. 


% todo: idee: adapter la preuve de stochastic gradient descent a stochastic mirror descent. Exploiter un parralele entre ces deux preuves pour obtenir une preuve pour stochastic mirror descent dans le cas ou f est relatively smooth w.r.t. A*. ce qu'on veut, c'est une hypothese analogue a expected smoothness (see paper of nico loizou) pour le relatively smooth.
% SGD: General Analysis and Improved Rates gower et al -- section 2 and 3 see equation (9) in particular (expected smoothnesS)
% Relatively-Smooth Convex Optimization by First-Order Methods, and Applications,  -- Section 3

\subsection{Convergence of Mirror Descent}
To get the descent lemma, apply successively relative smoothness, 3 points lemma, and relative strong convexity on $f$. Note that the strong convexity constant $m$ may be zero. Recall that $\ell_f(\natp, \natp^{t}) = f(\natp^t) + \langle \nabla f (\natp^t) ; \natp - \natp^t \rangle $ is the linearization of $f$ at $\natp^t$ evaluated in $\natp$. 
\begin{align}
	f(\natp^{t+1}) 
	&\leq \ell_f(\natp^{t+1}, \natp^{t})  + L \bregman(\natp^{t+1}; \natp^{t})
	& \text{(Relative $L$-smoothness)} \\
	& \leq \ell_f(\natp^{*}, \natp^{t})  + L \bregman(\natp^{*}; \natp^{t}) -L \bregman(\natp^{*}; \natp^{t+1}) 
	& \text{(3 Points Lemma w. $x=\natp^*$)} \\
	& \leq f(\natp^*) + (L - m) \bregman(\natp^{*}; \natp^{t}) -L \bregman(\natp^{*}; \natp^{t+1}) 
	& \text{(Relative $m$-strong convexity)}
\end{align}
Applying the 3 points lemma with \TODO{f}

\section{Self-Concordance}
\subsection{Motivation}
A big problem is that $\conj$ is seldom Lipschitz or smooth. For instance the log-partition function of a multivariate normal is 
\begin{align}
    \logpart(\eta, \Lambda) = \half \eta^\top \Lambda^{-1} \eta - \log\det(\Lambda)
\end{align}
which is defined on $\eta \in \real^d$ and $\Lambda\in \real^{d\times d}$ symmetric positive definite. 
It is not strongly convex, so $\conj$ is not smooth.

Another hypothesis that may be more suitable is self-concordance. $f: \real \rightarrow \real$ is SC if 
\begin{align}
    \abs{f'''(x)} \leq 2 f''(x)^{\frac{3}{2}} \; .
\end{align}
The exponent $\frac{3}{2}$ is motivated by dimensional analysis and the factor $2$ appears to simplify downstream calculus.
A multidimensional function $f: \real^n \rightarrow \real$ is SC if it's restriction to any line is SC.
Negative logarithm $-\log(x)$ and entropy $x\log(x)$ are both self-concordant function. This is good news for us since log-partition function may include SC logarithmic barriers.
In particular, gaussians have a logarithmic term. They also have an inverse term which is not self-concordant, but which is generalized self-concordant.

\subsection{Suboptimality and Newton Decrement}
An important property of self-concordant functions (cite Boyd's book, although Nesterov's may be better) is that their suboptimality  may be upper bounded by the Newton Decrement 
\begin{align}
    \decrement(x)^2 = \nabla f(x)^T \nabla^2 f(x)^{-1} \nabla f(x) \; .
\end{align}
In general, subtracting the minimum $f^*$ of $f$ , we have
\begin{align}
    f(x) - f* \leq - \decrement(x) - \log( 1- \decrement(x)) \; .
\end{align}
Note that this bound is vacuous for $\decrement(x)\geq 1$. For $y = \decrement(x) \leq 0.68$, we have $ - y - \log(1-y) \leq y^2$, so we get the bound
\begin{align}
    f(x) - f^* \leq \decrement(x)^2 = \nabla f(x)^T \nabla^2 f(x)^{-1} \nabla f(x) \; .
\end{align}

Our functions of interest is $f(\natp)= \bregman (\natp ; \natp^*)= \bregmanconj ( \meanp^* ; \meanp) = g(\meanp)$, with minimum $f^*=0$. 
If $\logpart$ is self-concordant, then so is $f$, but not necessarily $g$.
The gradient and Hessian of $f$ are
\begin{align}
    f(\natp) 
    &= \logpart(\natp) - \logpart(\natp^*) - \langle \meanp^*, \natp - \natp^* \rangle \\
    \nabla f(\natp) 
    &= \meanp - \meanp^* = \expect[p(X|\natp)]{T(X)} - \expect[p(X|\natp^*)]{T(X)} \\
    \nabla^2 f(\natp) 
    &= \mSigma(\natp) = \Cov_{p(X|\natp)}[T(X)]
\end{align}
so that we get the bound.
\begin{align}
    \bregmanconj(\mu^* ; \mu) 
    \leq \decrement(\natp)^2 
    = \norm{\mu^* - \mu(\theta)}^2_{\mSigma(\theta)^{-1}} 
    \leq 0.46
\end{align}
% If we were to use $g(\meanp)$ instead, we would get dirty third order derivatives
% \begin{align}
%     g(\meanp) 
%     &= \conj(\meanp^*) - \conj(\meanp) - \langle \natp, \meanp^* - \meanp \rangle \\
%     \nabla g(\meanp) 
%     &= -\natp - \nabla^2 \conj(\meanp) (\meanp^* - \meanp) + \natp \\
%     &= \nabla^2 \conj(\meanp) (\meanp - \meanp^*) \\
%     \nabla^2 g(\meanp) 
%     &= \nabla^2 \conj(\meanp) + \nabla^3 \conj(\meanp ) (\meanp - \meanp^*)
% \end{align}
% where the last expression involves a third order tensor.

Finally, if instead we were looking at a different function switching the role of $\meanp$ and $\meanp^*$,  $h(\meanp) = \bregmanconj ( \meanp ; \meanp^*)$, then we would get
\begin{align}
    \nabla h(\meanp ) 
    &= \natp - \natp^* \\
    \nabla^2 h(\meanp ) 
    &= \nabla^2 \conj(\meanp) 
    = \nabla^2 \logpart(\natp) ^{-1}
    = \Cov_{p(X|\natp)}[T(X)]^{-1}\\
    \implies 
    \decrement(\meanp)
    &= \Var_{p(X|\natp)}[(\natp - \natp^*)^T T(X)] \; .
\end{align}
This is just a remark. I don't think it can help us to get anywhere. 

\section{Related Work}
Exponential families are a mainstream tool in machine learning. They are used  to generalize linear regression \citep{mccullagh1989generalized}, PCA \citep{collins2001generalization} or k-means \citep{banerjee2005clustering} to diverse data types and distributions.
\citet[Chapter 3]{wainwright2008graphical} give a great overview of exponential families and their dual structure. 

\citet{agarwal2010geometric} highlight that the MAP problem of exponential families is also a Bregman median problem. They then use this equivalence to justify the use of conjugate prior in Bayesian estimation and hybrid generative-discriminative modeling.
Note that this equivalence holds only if sufficient statistics are within $\dom \conj  = \mathrm{Im} (\nabla\logpart)$, a point ignored by authors yet critical as it does not hold for multivariate normals with unknown covariance. 
\citet{raskutti2015information} highlight the equivalence between mirror descent in $\natp$ and natural gradient descent in $\meanp$ (or vice-versa) for the exponential family. Then they use the optimality result by Shunichi Amari on Natural Gradient Descent to show that Mirror Descent iterates reach the Cramer-Rao lower bound.

\citet{kunstner2020homeomorphic} show with an exquisite elegance that Expectation-Maximization in exponential families can be cast as mirror descent on the negative log-likelihood. Then they use recent mirror descent rates based on the idea of relative smoothness \citep{birnbaum2011distributed, bauschke2017descent, lu2018relatively} to show convergence of EM in KL-divergence, including for the ubiquitous Gaussian Mixture model, which had resisted bounding attempts for 50 years.

\citet{pfau2013generalized} gives a bias variance decomposition for Bregman divergences. However it does not give interesting conclusions for our problem of interest.

\citet{ostrovskii2021finite}  shows a convergence rate for the MLE (or other $M$-estimators)   when  the log-likelihood is self-concordant. This convergence rate holds only after a given number of samples have been seen. This is because they use the fact that self-concordant losses are upper bounded by a quadratic in a neighborhood of the optimum. 

\subsection{Alternative Research Tracks}
\begin{itemize}
	\item Weakly supervised : missing part of $x_i$ for each $i$.
	\item Structured prediction : graphical model on $X$ or other kind of structure.
\end{itemize}



\bibliographystyle{apalike}
\bibliography{references.bib}


\newpage
\appendix

\section{Cramer-Rao Lower Bound}
In statistics, there is this famous theorem known as the Cramer-Rao lower bound. In its simplest form, it states that the covariance of an unbiased estimator of a distribution's parameter $\natp$ is lower bounded by the inverse Fisher information matrix, divided by $n$
\begin{align}
    \Cov(\hat \natp_n)  
    &\succeq \inv{n} \cI(\natp_*)^{-1} \\
    \iff
    \expect[\cD\sim \natp_*]{(\hat \natp_n - \natp_*)(\hat \natp_n - \natp_*)^T}
    &\succeq \inv{n} \expect[X\sim \natp_*]{-\nabla^2 \log p(X|\natp_*)}^{-1} \; .
\end{align}
This is a typical statistic bound, which is concerned with the $\ell^2$ accuracy of an estimator, via its covariance. In contrast, here we are concerned with the quality of the estimator in terms of KL-divergence, eg "how accurately am I modeling this distribution?"

Note that in the case of an exponential family, the lower bound is always reached for the MLE estimator $\hat \mu_n$, because $\cI(\mu) = \nabla^2 \conj(\mu) = \nabla^2 \logpart(\natp)^{-1} = \Cov(T(X))^{-1}$, where the first step takes some calculus. For the estimator $\hat \natp_n$, this lower bound is not trivial.

\section{Posterior Expected Loss}

A conjugate prior for the exponential family with sufficient statistic $T(X)$ and natural parameter $\natp$ is 
\begin{align}
    \pi(\natp) 
    \propto \exp(-n_0 \bregman(\natp ; \natp_0)) 
    \propto \exp(-n_0 \logpart(\natp) + \langle n_0 \mu_0 ; \natp \rangle)
\end{align}
so we see that $\pi$ is part of the exponential family, with sufficient statistic $(\natp, \logpart(\natp))$ and natural parameters $(n_0 \mu_0 , -n_0)$. When we update this prior with Bayes formula on dataset $\cD=(x_1,\dots,x_n)$, we get a posterior $\pi(\natp | \cD)$ with natural parameters $(n_0 \mu_0 + \sum_i T(x_i) , -(n_0 + n))$. This is well detailed in \citet{agarwal2010geometric}. 

Given this posterior, a natural quantity to consider is the expected loss. Thus we might ask "what is the expectation of the posterior expected suboptimality?"
\begin{align}
    \expect[\cD\sim\natp_*]{\expect[\pi(\natp|\cD)]{\bregman(\natp ; \natp_*)}}
\end{align}
This quantity is even harder to estimate than the expected MAP suboptimality, because of the double integral. We can get a closed form for the age-old $\ell^2$ loss of $\cN(\mu, 1)$, which yields the exact rate $\inv{n}$. This is exactly twice the expected loss of the MLE.

\begin{example}
	We can also get a closed form for the posterior of a centered gaussian variance 
	\begin{align}
		\expect[\pi(\natp|\cD)]{\bregman(\natp ; \natp_*)} = \bregman(\hat \natp_n ; \natp_*) + \frac{2}{n} \frac{\mu_*}{\hat \mu_n} + \log\frac{n}{2} - \psi(1 + \frac{n}{2})
	\end{align}
	using the mean of the \href{https://en.wikipedia.org/wiki/Inverse-gamma_distribution}{inverse Gamma distribution} and the \href{https://en.wikipedia.org/wiki/Gamma_distribution#Logarithmic_expectation_and_variance}{logarithmic expectation} of a Gamma. The using results from the MLE, we get the exact formula
	\begin{align}
		\expect[\cD\sim\natp_*]{\expect[\pi(\natp|\cD)]{\bregman(\natp ; \natp_*)}}
		= \frac{2}{n} + \frac{2}{n(n-2)} + \psi(\half[n]) - \psi(1+ \half[n])
	\end{align}
	and we bound it to 
	\begin{align}
		\inv{2n} + \frac{4}{n(n-2)}
		\leq \expect[\cD\sim\natp_*]{\expect[\pi(\natp|\cD)]{\bregman(\natp ; \natp_*)}}
		\leq 
		\inv{n} + \frac{4}{n(n-2)}
	\end{align}
	which is about twice the MLE bounds~\eqref{eq:MLE_rate}, exactly like as we observed for the gaussian mean.
\end{example}


\section{Three points lemma}
\label{app:3points}

Proofs of convergence for mirror descent with relative smoothness rely on a specific property of Bregman divergences, familiar to information geometry folks. If $x_+$ is solution to 
\begin{align}
	\min	_{x\in C} \overbrace{f(x) + \cB_h(x ; y)}^{\phi(x)}
\end{align}
where $C$ is a closed convex set, $f$ is a convex function, $\cB_h$ is the Bregman divergence induce by some convex function $h$ (we will drop the index, and use a semicolon instead of double bars to lighten the notation), and $y$ is some reference vector. Then 
\begin{align}
	\forall x, f(x) + \cB(x ;y) \geq f(x_+) + \cB(x ; x_+) +\cB(x_+ ; y) \; .
\end{align}
This property is an analog of the Pythagorean theorem for generalized projections (setting $f=0$ and $h=\|.\|^2$).

\begin{proof}
	The first order optimality condition is
	\begin{align}
		&\langle \nabla \phi (x_+) , x -  x_+ \rangle \geq 0, \forall x \\
		=& \langle \nabla f(x_+) + \nabla h(x_+) - \nabla h (y), x - x_+ \rangle  
	\end{align}
	This proof relies on another property called three point property 
	\begin{align}
		\langle \nabla h(x_+) - \nabla h (y), x - x_+ \rangle  =\cB(x;y) -\cB(x;x_+) - \cB(x_+;y) 
	\end{align}
	which can be proved by expanding the right hand side.
	By convexity of $f$ we also have 
	\begin{align}
		 f(x_+) + \langle \nabla f(x_+), x - x_+ \rangle \leq f(x) \; .
	\end{align} 
	Putting it all together we get
	\begin{align}
		0 
		&\leq  \langle \nabla f(x_+), x - x_+ \rangle + \langle \nabla h(x_+) - \nabla h (y), x - x_+ \rangle  \\
		&\leq f(x) - f(x_+) + \cB(x;y) -\cB(x;x_+) - \cB(x_+;y)
	\end{align}
	which concludes the proof.
\end{proof}

\section{Vivien's approach}
\subsection{A big calculus in this specific setting}

Indeed we know the distribution of $\mu$ it is linked with the chi-square distribution. We also know the expression of the excess of risk. So we could do a big calculus to get the exact expression of the averaged excess of risk.

Recall the expression of the Gamma integral
\begin{align}
  \Gamma(\alpha):=\int_0^\infty t^{\alpha-1} \exp(-t) d t
\end{align}
Recall also that
\begin{align}
    \int x^q \exp(-nx^p) dx
    &= n^{-(q+1)/p} \int (n^{1/p}x)^q \exp(-(n^{1/p}x)^p) d(n^{1/p}x)\\
    &= n^{-(q+1)/p} \int u^q \exp(-u^p) du\\
    &= n^{-(q+1)/p} \int t^{q/p} \exp(-t) d(t^{1/p})\\
    &= n^{-(q+1)/p} \int p^{-1} t^{(q+1)/p - 1} \exp(-t) dt\\
    &= n^{-(q+1)/p} p^{-1} \Gamma((q+1)/p).
\end{align}
Finally if there is problem in zero with the logarithm appearing in Eq. (75, at time of writing), we can have a Taylor expansion of the log 
\[
  \log(x) = \sum_{i=1}^\infty \frac{(-1)^{i+1}(x-1)^i}{i},
\]
and show convergence with some Fubini's theorem on series-integral.

\subsection{Generic approach}

In this subsection, I detailed a generic approach to get non-asymptotic rates.
It is based on the calibration inequality Eq. (18, at time of writing).
The next step would be to detailed those derivations and have precise results for the specific case of only looking at the variance.

\paragraph{Calibration inequality.}
One of the main difficulty of the proof is that $\theta_n = (\nabla A)^{-1}(\mu_n)$ can be really big, meaning that we can not apply directly ``calibration inequality'' between the Bregman divergence ${\cal B}_{A^*}(\mu^*; \mu)$ and $\norm{\mu - \mu^*}^2$.
However, if we restrict ourselves to the domain $\theta'\in {\cal B}(0, R)$ with $R = \max(\norm{\theta}, \norm{\theta^*})$, we have that $A$ is $2^{-1}R^{-2}$-strongly convex. Therefore using Eq.~(18, at the time of writing), we have 
\begin{align}
    {\cal B}_{A^*}(\mu^*; \mu) 
    &\leq 2^{-1}R^{-2} \norm{\mu^* - \mu}^2\\
    &= 2 \max(\norm{\theta}, \norm{\theta^*})^2\norm{\mu^* - \mu}^2\\
    &= 2 \max(\norm{(\nabla A)^{-1}(\mu)}, \norm{(\nabla A)^{-1}(\mu^*)})^2\norm{\mu^* - \mu}^2.
\end{align}
With will proceed with this equation, we might get better equations. For example by considering a constant of strong convexity on the segment $[\norm{\theta}, \norm{\theta^*}]$ rather than the ball ${\cal B}(0, R)$.
The behavior of the max, will be similar to the behavior of the sum of the two elements since $\max(a, b) \leq a + b \leq 2\max(a, b)$, therefore, we proceed
with
\begin{align}
    {\cal B}_{A^*}(\mu^*; \mu) 
    &\leq 2 \norm{(\nabla A)^{-1}(\mu^*)}^2 \norm{\mu^* - \mu}^2
    + 2 \norm{(\nabla A)^{-1}(\mu)}^2 \norm{\mu^* - \mu}^2.
\end{align}
The first term is easy to control in expectation, see Eqs. (19-20, at time of writing), it is $\Var{T(X)} / n$. It is really the second part that we want to study. 
Because of the structure $\norm{(\nabla A)^{-1}(\mu)} \simeq \norm{\mu}^{-1}$
We have to take care of the event ``$\mu = n^{-1} \sum_{i\leq n} T(X_i)$ too small''.
We know that $\mu$ concentrate towards $\mu^*$ so this event should not happen too often.
So we want to study $\norm{(\nabla A)^{-1}(\mu)}^2 \norm{\mu - \mu^*}^2$.

\paragraph{Tail bound or average excess of risk?}
If we think with concentration inequality, we will get tail bound on the excess of risk, and not an average excess of risk.
However, recall that
\begin{align}
  \E[X] = \E[\int_{0}^{\infty} 1_{X > t} dt]
  = \int_{0}^\infty \E[1_{X>t}] dt
  = \int_0^\infty \mathbb{P}(X > t) dt.
\end{align}
So we can translate tail bound into average.

\paragraph{Concentration of a product.}
We want to study $\norm{(\nabla A)^{-1}(\mu)}^2 \norm{\mu - \mu^*}^2$, it is a product of two term, we can proceed with the fact that if $A$ and $B$ are two random variables from a sample space $(\Omega, \mu)$ to $\mathbb{R}$, we have
\[
    \{ \omega \in \Omega \vert A(\omega) B(\omega) > t\}
    \subset 
    \{ \omega \in \Omega \vert A(\omega) > s\}
    \cup
    \{ \omega \in \Omega \vert A(\omega) > t/s\}
\]
Which results from the fact that if $ab > t$ than either $a > s$, either $b > t/s$.
In our case, it means that for any $t > 0$, we have the deviation bound
\begin{align}
  \mathbb{P}(\norm{(\nabla A)^{-1}(\mu)}^2 \norm{\mu - \mu^*}^2 > t)
  &\leq \inf_{s>0} \mathbb{P}(\norm{(\nabla A)^{-1}(\mu)}^2 > 1/s)
  + \mathbb{P}(\norm{\mu - \mu^*}^2 > st)\\
  &= \inf_{s>0} \mathbb{P}(\norm{(\nabla A)^{-1}(\mu)} > s^{1/2})
  + \mathbb{P}(\norm{\mu - \mu^*} > (st)^{1/2}).
\end{align}
So we are left with the study of 
\begin{align}
  \mathbb{P}(\norm{\mu - \mu^*} > t)
  \qquad\text{and}\qquad 
  \mathbb{P}(\norm{(\nabla A)^{-1}(\mu)} > t).
\end{align}

\paragraph{Concentration on $\mu - \mu^*$.}
Most likely $T(X)$ is sub-Gaussian (do hand-derivations or look at works like \url{https://arxiv.org/pdf/1011.3027.pdf} or \url{https://arxiv.org/pdf/1902.03736.pdf}) and Bernstein concentration inequality leads to a tail bound of the type, with $\sigma^2$ a variance parameter 
($\sigma^2 = \Var_{\theta^*}(T(X))$)
\begin{align}
    \mathbb{P}_{(X_i)_{i\leq n}}(\norm{\mu - \mu^*} > t) \leq \exp(-n t^2 / \sigma^2).
\end{align}
Normally, in Bernstein inequality there is an additional term in $n$, but the convergence rates will not depend on this additional term, so I am omitting it for simplicity.
Note that when we will integrate that we will get
\begin{align}
    \int_0^\infty \mathbb{P}_{(X_i)_{i\leq n}}(\norm{\mu - \mu^*} > t^{-1/2}) dt
    &=\int_0^\infty \exp(-n t / \sigma^2) dt\\
    &= \sigma^2 n^{-1}\int_0^\infty \exp(- nt / \sigma^2) d(nt\sigma^{-2})\\
    &= \sigma^2 n^{-1} \int_0^\infty \exp(-x) dx =\sigma^2 n^{-1}.
\end{align}

\paragraph{Probability that $\mu$ is close to zero.}
Intuitively, we should study $\mathbb{P}(\norm{(\nabla A)^{-1}(\mu)} > t)$, which should relates to $\mathbb{P}(\norm{\mu} < t)$, which can be express exactly as we know the cumulative distribution function of the chi-square and that $\mu$ is a variant of the chi-square.
It seems to me that here we will not get optimal rates, because the calibration inequality is too harsh when $\mu$ goes to $0$, as I develop in the next paragraph.

\subsection{Big picture.}
Intuitively, we have summarize in the part in $\mathbb{P}(\norm{(\nabla A)^{-1}(\mu)} < t)$, the problem that arise with ${\cal B}_{A^*}(\mu^*; \mu)$ when $\mu$ is small. 
We could make this intuition more formal with
\begin{align}
   {\cal B}_{A^*}(\mu^*; \mu)
   &= 1_{\norm{\mu} \geq s} {\cal B}_{A^*}(\mu^*; \mu)
   + 1_{\norm{\mu} < s} {\cal B}_{A^*}(\mu^*; \mu)\\
   &\leq \sup_{\norm{\mu'} \geq s}(\norm{(\nabla A)^{-1}(\mu')})^2 \norm{\mu - \mu^*}^2
   + 1_{\norm{\mu} < s} {\cal B}_{A^*}(\mu^*; \mu).
\end{align}
Suppose the $\sup$ is finite, the first term is easy to control.
The second term is what we should take care of. We have
\begin{align}
    1_{\norm{\mu} < s} {\cal B}_{A^*}(\mu^*; \mu) 
    &=  1_{\norm{\mu} < s} (A^*(\mu^*) - A^*(\mu) - <\nabla A^*(\mu), \mu^* - \mu>)\\
    &\leq  1_{\norm{\mu} < s} (A^*(\mu^*) - \inf_{\norm{\mu'} < s} A^*(\mu') + \norm{\nabla A^*(\mu)} \sup_{\norm{\mu''} < s} \norm{\mu^* - \mu''}).
\end{align}
We know that, under some strict convexity condition, $\nabla A^* = (\nabla A)^{-1}$ (similar to Eq. (25) at time of writing). Therefore we really have to study, with harsh notation
\begin{align}
    \int_{0}^s \norm{(\nabla A)^{-1}(\mu)} d\mathbb{P}(\norm{\mu} < t)
    =\int_{0}^s \norm{(\nabla A)^{-1}(\mu)} \mathbb{P}(\norm{\mu} \in [t, t+dt]).
\end{align}
It seems to me that with the precedent development we were implicitly studying
\begin{align}
    \int_{0}^s \norm{(\nabla A)^{-1}(\mu)}^2 d\mathbb{P}(\norm{\mu} < t),
\end{align}
which of course is worse.
The study can be done precisely as we know the distribution of $\mu$.
In terms of rates, in my current view, this path is tight if and only if $<\nabla A^*(\mu), \mu^* - \mu>$ do behave like $\norm{(\nabla A)^{-1}(\mu)}$ towards $0$. In terms of constants, we have done pretty harsh majoration in order to get back to $c\norm{\mu-\mu^*}$ with constants $c$ that are clearly suboptimal.

\paragraph{Tail bound or average risk?}
Note that, if $\mu$ is sub-Gaussian concentrating towards $\mu^*$,
we can give a simpler result than average risk, 
which is that we can that that 
with proba $1 - \mathbb{P}(\norm{\mu} < 1/2\norm{\mu^*}) = 1 - \exp(-c\norm{\mu^*}^2 n)$, 
we have a tail bound $\mathbb{P}(\norm{\mu}^{-2}\norm{\mu - \mu^*}^2 > t) = \exp(-cnt / \norm{\mu^*})$.
This means that we can say that 
with proba $1 - \exp(-c\norm{\mu^*}^2 n) - \delta$ for any $\delta >0$, 
we have that $B_{A^*}(\mu^*; \mu) < cn^{-1}\log(1/\delta) / \norm{\mu^*}$.


\section{Fenchel conjugate motivation to self-concordance}

In the most regular case, when $f(x)$ is a convex function, continuously differentiable on its domain, then its convex conjugate $f^*(y) = \max_x \langle x, y \rangle - f(x)$ verifies
\begin{align}
    \nabla f \circ \nabla f^*  &= \Id \\
     \nabla f^* \circ \nabla f &= \Id
\end{align}
where $\Id$ is the identity function on the relevant domain. In words, the gradients of $f$ and $f^*$ are reciprocal.
Deriving this equality yields
\begin{align}
    \nabla^2 f(x) \nabla^2 f^*(x^*) = I_n
\end{align}
where $x,x^*$ are conjugate points -- e.g. $x^*=\nabla f(x)$ and $x = \nabla f^*(x^*)$.
Now, it gets interesting to us when we derive again this equality. Let's tackle the 1D case first
\begin{align}
    &f''(x) f^{* \prime \prime}(f'(x)) = 1, \forall x \\
    \implies
    &f'''(x)f^{* \prime \prime}(f'(x))  + f''(x)^2 f^{* \prime \prime \prime}(f'(x))  = 0 \\
    \implies
    &\frac{f'''(x)}{f''(x){\frac{3}{2}}}  + \frac{f^{* \prime \prime \prime}(x^*)}{f^{* \prime \prime}(x^*){\frac{3}{2}}}  = 0
\end{align}
where to get to the last line we used the first line, and we divided the second line by $f''(x)^{\frac{1}{2}}$. 
We see that for a pair of conjugate functions, the self-concordance ratio is preserved, modulo the sign.
This gives another rational, beyond dimensional analysis, for using this ratio as a regularity assumption for convex analysis. 

It is also very helpful for us, since we are looking at pairs $\logpart, \conj$, and their associated Bregman divergences. 
If $\logpart$ is SC, then so is 
$f(\natp)= \bregman(\natp ; \natp^*) = \logpart(\natp) - \langle \meanp^*, \natp \rangle + \cst$. 
And $\conj$ is SC as well, thus  $h(\meanp) = \bregmanconj ( \meanp ; \meanp^*)$ is SC.
But there is no reason for 
$g(\meanp) = \bregmanconj ( \meanp^* ; \meanp) =  \cst - \conj(\meanp) - \langle \nabla \conj (\meanp), \meanp^* - \meanp \rangle $ to be SC. 

The multivariate generalization of this formula is a third order tensor equality
\begin{align}
    \nabla^2 f^{-\half} \nabla^3 f \nabla^2 f^{-1} + \nabla^2 f^{* -\half} \nabla^3 f^* \nabla^2 f^{* -1} = 0
\end{align}
where we omit  multiplication axis and functions take relevant argument $x$ or $x^*$. Consequently, a multivariate definition of self-concordance might take the form of an inequality on the 3d tensor $\nabla^2 f^{-\half} \nabla^3 f \nabla^2 f^{-1}$.



\section{MAP on Graphs}
Assume that the variable $X$ factors along some graph $G$. 
We write $G(i)$ the parents of $X_i$ in $G$. 
Then we model the conditional distribution of $X$ given parameter vector $\theta$ factors as
\begin{align}
    p(X|\natp) = \prod_i p(X_i | X_{G(i)};\natp_i)
\end{align}
where $\natp_i$ is the parameter associated to the mechanism $X_{G(i)} \rightarrow X_i$. 
Embracing the Bayesian viewpoint, the independent mechanism principle is embodied as independence between parameters
\begin{align}
    p(\natp) = \prod_i p(\natp_i) \; .
\end{align}
% In other words, we have the following assumptions on $(\natp,X)$
% \begin{align}
%     \indep_{i=1}^d \natp_i \\
%     \natp_i \indep X_{G(i)}
% \end{align}
Following these equations, the joint distribution on $(\natp,X)$ factors along a larger graph $G'$ which augments $G$ by adding nodes $\natp_i$ with arrows pointing to $X_i$, as illustrated in Figure~\ref{fig:joint_graph}.
\begin{figure}[ht]
    \centering
    \begin{tikzpicture}[shorten >=1pt,->]
      \tikzstyle{vertex}=[draw, circle,minimum size=24pt,inner sep=0pt]
      \foreach \name/\txt/\x/\y in {x1/X_1/0/0, x2/X_2/3/0, x3/X_3/2/-1.5, t1/\theta_1/-1.5/-0.5, t2/\theta_2/4.5/-0.5, t3/\theta_3/0.5/-2}
        \node[vertex] (G-\name) at (\x,\y) {$\txt$};
    
      \foreach \from/\to in {x1/x2, x2/x3, x1/x3, t1/x1, t2/x2, t3/x3}
        \draw (G-\from) -- (G-\to);
    \end{tikzpicture}
    \caption{A graph $G'$ factorizing $(\natp,X)$. Although the graph restricted on $X$ does not encode any conditional independence, $G'$ does on the joint distribution.}
    \label{fig:joint_graph}
\end{figure}
With such a graph, the Bayesian posterior can be factorized as well
\begin{align}
    p(\natp|X) 
    &\propto  p(X|\natp) p(\natp)& \\
    & = \prod_i p(X_i | X_{G(i)};\natp_i) p(\natp_i)
    &(\indep_i \natp_i) \\
    & = \prod_i p(X_i | X_{G(i)};\natp_i) p(\natp_i\violet{|X_{G(i)}})
    &(\natp_i \indep X_{G(i)}) \\
    & = \prod_i p(X_i, \violet{\natp_i} | X_{G(i)})    &  \\
    & = \prod_i p(\natp_i| \violet{X_i}, X_{G(i)})p(X_i|X_{G(i)})    &  \\
    \implies p(\natp|X) 
    &= \prod_i p(\natp_i| \violet{X_i}, X_{G(i)}) \; .
\end{align}
In words, a consequence of the independence mechanism principle is that the posterior distribution of $\natp_i$ can be inferred solely from $X_i$ and its parents. 

\subsection{Equality of directions for 2 categorical variables}
In my paper on the analysis of causal speed, I proved the equivalence between sampling a joint distribution $\omega = p(A,B) \in \simplex_{K\times K}$ on $(A,B)\in \{1,\dots,K\}^2$ from a Dirichlet with parameter $\gamma\in \real_+^{K\times K}$ and sampling independently the marginal distribution $\mu = p(A)\in \simplex_K$ and the conditional distributions $\nu_i = p(B|A=i)\in\simplex_K$ from Dirichlets with respective parameters $\sum_{j=1}^K \gamma_{:,j} = \gamma \ones$ (matrix vector product) and $\gamma_{i,:}$
\begin{align}
    \underbrace{\Dir_{K^2}((\gamma_{i,j})_{i,j=1}^K) }_{p(\omega)}
    \equiv 
    \underbrace{\Dir_K\left( \gamma \ones \right)}_{p(\mu)} 
    \otimes \left (\bigotimes_{i=1}^K 
    \underbrace{\Dir_K((\gamma_{i,j})_j)}_{p(\nu_i)} 
    \right ) 
    \label{eq:dirichlet_factors}
\end{align}
Seeing data samples $(\cA,\cB) = (A_i,B_i)_{i=1}^n$ as one-hot encodings in $\real^K\times \real^K$, the posterior reads
\begin{align}
p(\mu|\cA) &= \Dir(\gamma\ones + \sum_i A_i)\\
p(\nu_k|\cA,\cB) &= \Dir(\gamma_{k,:} + \sum_i  A_{i,k} B_i ) \\
p(\omega | \cA,\cB) & = \Dir( \gamma + \sum_i A_i B_i^\top )  
\end{align}
where $A_i B_i^\top$ is the one hot matrix encoding of $A,B$.
These three posteriors are obtained independently of each other following rules of calculus for Dirichlet distributions. Yet they happen to define the same distribution on distributions, as we verify below with the two equalities from equation \eqref{eq:dirichlet_factors}.
\begin{align}
    \left(\gamma + \sum_i A_i B_i^\top \right)_{k,l} 
    &= \left(\gamma_{k,:} + \sum_i  A_{i,k} B_i \right)_l \\
    \left(\gamma + \sum_i A_i B_i^\top \right) \ones 
    &= \gamma \ones + \sum_i A_i \; .
\end{align}
The interpretation of this result is that \emph{taking the posterior with the decomposition $A\rightarrow B$ or $B\rightarrow A$ give the same result}. As a corollary the MAP is also the same
\begin{align}
\hat \omega^\text{MAP} 
= \frac{\gamma + \sum_i A_i B_i^\top}{\ones^\top (\gamma + \sum_i A_i B_i^\top) \ones}
=\frac{\gamma + \sum_i A_i B_i^\top}{n_0 + n}
\end{align}
Using Bayesian statistics with this prior, there is no distinction between directions.

Is this bound to happen with a symmetric prior ? Let's give a name to the change of variable $f(\omega) = \mu,\nu$. 
Remark that $f(\omega^\top) = \mu_\leftarrow, \nu_\leftarrow$, eg in the categorical special case transposing omega and changing variables give the anticausal direction. 
For sure  $p(X|\mu,\nu) = p(X|f(\omega)) = p(X|\omega)$. 
Using the change of variable formula  we get something.
But which equality am I looking for exactly ? 
\end{document}