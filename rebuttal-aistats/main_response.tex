% !TEX root = aistats2022_author_response.tex

\ifenablecomments{
	\color{red}
	\large Comments are enabled.
	Uncomment {\tt \textbackslash  enablecommentsfalse} for the submission.
}\fi


We thank all the reviewers for their careful review and insightful comments. We start our rebuttal by answering the common concern, followed by specific comments for each reviewer.
	
\paragraph{Common answer: Is the work relevant for AISTATS?}
We understand from the reviewers that “open problem papers”  are unusual, and the question of whether such an article should be accepted or not at AISTATS is legitimate. 
When writing the paper, we aimed to submit it at a cross disciplinary conference, as the paper’s topic is at the intersection of statistics and optimization. After a careful review of possibilities, AISTATS seems to be a perfect match, given what is written in the call of papers:
"AISTATS is an interdisciplinary gathering of researchers at the intersection of computer science, artificial intelligence, machine learning, statistics, and related areas. Since its inception in 1985, the primary goal of AISTATS has been to broaden research in these fields by promoting the exchange of ideas among them."  
We strongly believe our article will promote an exchange of ideas, as emphasized by some of the reviewers below:
(R3) "It provides an interesting viewpoint and novel (even if partial) results compared to what can be found in the literature."
(R5)  "The significance of the paper is that it provides insight into a deeper connection between exponential family MLE/MAP estimation and the Bregman geometry. [...]  The connections between self-concordance and the quadratic approximation to the Bregman was also fairly illuminating."
(R6) "The presented results (or lack of) are very relevant and useful to both optimisation and statistics communities, and should spark several discussions at the conference."

Additionally, members of the optimization and statistics community we talked to during our investigation of this problem were 
either convinced a result already exist in the other community, 
either interested in knowing the answers to these questions.
Motivated by these feedbacks, we decided to submit as is, to highlight the similarity between these questions, 
and to stimulate interest and get the help of the community to make progress on this hard problem.


\paragraph{(R1) The statement of the conjecture should be more precise.}
We know that asymptotically, the expected KL belongs to $\frac{d}{2 n} + O (n^{-\frac{3}{2}})$, so it is tempting to explicitly search for a non-asymptotic upper bound in this set. 
Yet, we are still unaware of any general upper bound.
In our opinion, a bound of the form $\frac{c}{n}$ with the wrong constant $c>\frac{d}{2}$, or even a bound $\frac{c}{\sqrt{n}}$ would be novel and interesting.
Consequently, we decided to keep the question open-ended.  
Additionally, about the statement: (R1) "many of such results are essentially known."
We are very interested in knowing examples of such   results, as we believe they could help us improve the paper.

\paragraph{(R2) The coverage of related work needs improvement.}
``some related work should be more thoroughly discussed. Currently the authors did a good job in reviewing the literature from optimization, but the counterpart on the statistical literature is less satisfactory''
\FDK{I think it's fair, but I don't know how to fix it (without a $\sim$3 weeks lit review and a new page)}
\RLP{Let's not address it as is, and focus on the specific references that were given by R2 and R3. Also add \url{https://arxiv.org/pdf/1811.11419.pdf} ?}

\paragraph{(R2 \& R5) Additional examples, beyond the Gaussian case, should be considered}
R2:
``When it comes to specific examples, in addition to the Gaussian model discussed extensively in the paper, some other models (mainly categorical models) should also be discussed.
[...] for categorical examples such as the multinomial model studied in
~\\
Kamath, S., Orlitsky, A., Pichapati, D., \& Suresh, A. T. (2015, June). On learning distributions from their samples. In Conference on Learning Theory (pp. 1066-1100). PMLR.
''

R2:
``For example, the KL divergence computation in the multinomial model or a product Poisson model is also a classical problem, which should be discussed more thoroughly.''

R5:
``While the MLE/MAP
estimates for exponential families are well-known, I would have liked if the
paper could comment on the potential generalizability of the MLE/MAP bound
found for the Gaussian, to other exponential family distributions.''

\FDK{This is fair, and should be relatively easy to add. No internet so couldn't check Kamath et al. (2015), but this must be out there.
I haven't delved into Bernoulli bounds (I was only plotting the actual expected losses because you could get it in closed form), but I can look into it this week.
}
\RLP{We are going to investigate categoricals, and if we get satisfying upper bound we will include it in the paper.}

\paragraph{(R5)}
Recall that D'Orazio (2021) proves linear convergence only down to a variance ball around the optimum.

\paragraph{(R6)} 
Thank you for your very encouraging comment.

\section*{Minor comments}

\subsection*{Recommended litterature}

R1:
``In a more general stochastic optimization setup, [...]  bound of (nearly) the desired form (but not restricted to exponential families) [...] 

Competing with the Empirical Risk Minimizer in a Single Pass (COLT 2015), Roy Frostig, Rong Ge, Sham M. Kakade, Aaron Sidford''

R3:
``other works use a similar mirror descent framework for the optimization of a parameter of an exponential family and might be relevant to cite:

The Information Geometry of Mirror Descent
Garvesh Raskutti, Sayan Mukherjee, 2013.

Conjugate-Computation Variational Inference : Converting Variational Inference in Non-Conjugate Models to Inferences in Conjugate Models. Mohammad Emtiyaz, Khan Wu Lin, 2017.'' 

\FDK{
Haven't read it, but I'd guess Frostig et al. (2015) is a generalization of SGD being asymptotically ``optimal'' on the test set, which wouldn't solve the problem. Good to cite though. 
For the other two, they are on, or use, mirror descent, so could be cited as related work.
But there's no convergence rate, whether stats nor optimization.
}

\subsection*{Other results to mention}

R2:
``The asymptotic rate d/2n is also a minimax lower bound of the KL divergence (not necessarily restricted to MLE/MAP). This is well-known in the information theory literature on the minimax prediction error/redundancy.''

\FDK{That should probably be added as a parenthetical somewhere}

