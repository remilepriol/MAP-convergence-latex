% !TEX root = aistats2022_author_response.tex

\ifenablecomments{
	\color{red}
	\large Comments are enabled.
	Uncomment {\tt \textbackslash  enablecommentsfalse} for the submission.
}\fi


\section*{Main criticisms}

\subsection*{It's an open problem paper}

R1: 
``This does not qualify as a complete research work.'' 

R2: 
``The major weakness is that this paper is not a "regular paper" in the usual sense. 
[...]
So I am not sure whether the very nature of the paper is suitable for aistats, or more suitable for other venues which have calls for open problems.''


\FDK{Paraphrasing SLJ;}
The main weakness noted by Reviewers 1 \& 2 
is that our submission aims to promote discussions around an open problem, 
rather than a ``regular paper'' (R2), 
and they are unsure whether it is suitable for the AISTATS audience.
We agree with this characterization of the submission. 
%and acknowledge that such papers are (perhaps wrongly) scarce in the major machine learning conferences.
Our submission to AISTATS was motivated by the emphasis, in the call for papers, on the promotion of the exchange of ideas between the communities working in statistics and optimization in the context of machine learning.

The people in optimization and statistics we talked to during our investigation of this problem where interested in knowing the answers to these questions; members of one community thought this problem must have already been solved by the other. This motivated our submission; to show the similarities between those questions, stimulate interest and get the help of the community to make progress on this hard problem.

\subsection*{The statement of the conjecture is not sufficiently precise}

R1: 
``Besides, for the open problems, it would be better to state the desired results in a concrete way.
[...]
This is a vague question, as there could be many forms of possible bounds, under various assumptions, and many of such results are essentially known.''

\FDK{This one is fair, and I'm not sure how to fix it. 
I would go with ``Under no additional assumption, other than steep exponential families, what is the generalization of the quadratic bias-variance bound?''. But this is still a question, rather than a conjecture about a concrete result that should hold.
}

\RLP{Re: ``and many of such results are essentially known'', what are those?
	\FDK{Any non-asymptotic result under additional assumptions fit the bill like the ones we cite by Bach et al. and others. I assume many special cases are much more well known in stats}
}

\subsection*{The coverage of related work needs improvement}

R2:
``some related work should be more thoroughly discussed. Currently the authors did a good job in reviewing the literature from optimization, but the counterpart on the statistical literature is less satisfactory''

\FDK{I think it's fair, but I don't know how to fix it (without a $\sim$3 weeks lit review and a new page)}

\subsection*{Additional examples, beyond the Gaussian case, should be considered}

R2:
``When it comes to specific examples, in addition to the Gaussian model discussed extensively in the paper, some other models (mainly categorical models) should also be discussed.
[...] for categorical examples such as the multinomial model studied in
~\\
Kamath, S., Orlitsky, A., Pichapati, D., \& Suresh, A. T. (2015, June). On learning distributions from their samples. In Conference on Learning Theory (pp. 1066-1100). PMLR.
''

R2:
``For example, the KL divergence computation in the multinomial model or a product Poisson model is also a classical problem, which should be discussed more thoroughly.''

R5:
``While the MLE/MAP
estimates for exponential families are well-known, I would have liked if the
paper could comment on the potential generalizability of the MLE/MAP bound
found for the Gaussian, to other exponential family distributions.''

\FDK{This is fair, and should be relatively easy to add. No internet so couldn't check Kamath et al. (2015), but this must be out there.
I haven't delved into Bernoulli bounds (I was only plotting the actual expected losses because you could get it in closed form), but I can look into it this week.
}

\section*{Minor comments}

\subsection*{Recommended litterature}

R1:
``In a more general stochastic optimization setup, [...]  bound of (nearly) the desired form (but not restricted to exponential families) [...] 

Competing with the Empirical Risk Minimizer in a Single Pass (COLT 2015), Roy Frostig, Rong Ge, Sham M. Kakade, Aaron Sidford''

R3:
``other works use a similar mirror descent framework for the optimization of a parameter of an exponential family and might be relevant to cite:

The Information Geometry of Mirror Descent
Garvesh Raskutti, Sayan Mukherjee, 2013.

Conjugate-Computation Variational Inference : Converting Variational Inference in Non-Conjugate Models to Inferences in Conjugate Models. Mohammad Emtiyaz, Khan Wu Lin, 2017.'' 

\FDK{
Haven't read it, but I'd guess Frostig et al. (2015) is a generalization of SGD being asymptotically ``optimal'' on the test set, which wouldn't solve the problem. Good to cite though. 
For the other two, they are on, or use, mirror descent, so could be cited as related work.
But there's no convergence rate, whether stats nor optimization.
}

\subsection*{Other results to mention}

R2:
``The asymptotic rate d/2n is also a minimax lower bound of the KL divergence (not necessarily restricted to MLE/MAP). This is well-known in the information theory literature on the minimax prediction error/redundancy.''

\FDK{That should probably be added as a parenthetical somewhere}

