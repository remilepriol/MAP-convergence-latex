% !TEX root = aistats2022_author_response.tex

\ifenablecomments{
	\color{red}
	\large Comments are enabled.
	Uncomment {\tt \textbackslash  enablecommentsfalse} for the submission.
}\fi


We thank all the reviewers for their careful review and insightful or encouraging comments. We start our rebuttal by answering the common concern, followed by specific comments for each reviewer.
	
\paragraph{Common answer: Is the paper relevant to AISTATS?}
We understand from the reviewers that “open problem papers”  are unusual, and the question of whether such an article should be accepted or not at AISTATS is legitimate. 
When writing the paper, we aimed to submit it at a cross disciplinary conference, as the paper’s topic is at the intersection of statistics and optimization. After a careful review of possibilities, AISTATS seems to be a perfect match, given what is written in the call of papers:
"AISTATS is an interdisciplinary gathering of researchers at the intersection of computer science, artificial intelligence, machine learning, statistics, and related areas. Since its inception in 1985, the primary goal of AISTATS has been to broaden research in these fields by \emph{promoting the exchange of ideas} among them."  
We strongly believe our article will promote an exchange of ideas, as emphasized by some of the reviewers below:
(R3) "The paper tackles an important problem [...] that is surely of interest for the statistics/ML community."
(R5)  "The significance of the paper is that it provides insight into a deeper connection between exponential family MLE/MAP estimation and the Bregman geometry. [...]  The connections between self-concordance and the quadratic approximation to the Bregman was also fairly illuminating."
(R6) "The presented results (or lack of) are very relevant and useful to both optimisation and statistics communities, and should spark several discussions at the conference."

In 1954, John Tukey  \nocite{tukey1954unsolved} wrote "Difficulties in identifying problems have delayed statistics far more than difficulties in solving problems. This seems likely to be the case in the future, too."
Many members of the optimization and statistics community we talked to during our investigation of this problem thought: "Surely, this problem has been solved."
Motivated by these feedbacks, we decided to submit as is, 
to stimulate interest and get the help of the community to make progress on this hard problem.


\paragraph{(R1) The statement of the conjecture should be more precise.}
We know that asymptotically, the expected KL belongs to $\frac{d}{2 n} + O (n^{-\frac{3}{2}})$, so it is tempting to explicitly search for a non-asymptotic upper bound in this set. 
Yet, we are still unaware of any general upper bound.
In our opinion, a bound of the form $\frac{c}{n}$ with a large constant $c>\frac{d}{2}$, or even a bound $\frac{c}{\sqrt{n}}$ would be novel and interesting.
Consequently, we decided to keep the question open-ended.  

Additionally, about the statement: (R1) "many of such results are essentially known."
We are very interested in knowing examples of such   results, as we believe they could help us improve the paper.

We will also discuss \citet{frostig2015competing}, noting that they work with smooth and strongly convex losses, which is precisely the assumptions we aim to relax.

\paragraph{(R2) The statistical literature review should be more detailed.}
We understand R2 is mostly talking about the papers mentioned in the locally quadratic subsection. 
As requested, we will expand the description of these references to convey a sharper picture of the state-of-the-art
We will also add the reference you provided that proves minimax bound for categorical distributions with $\ell^1, \ell^2$ and $\chi^2$ distances, or for all $f$-divergences on a subset of the simplex. 

\paragraph{(R3)} We will discuss the references you provided. In \S6.2.1, we misleadingly asserted that 32 is weaker than 31. While this is true for quadratics, it may not be true in general. (And tilde g was a typo.)

\paragraph{(R5)} We would like to clarify that \citet{dorazio2021stochastic} proves linear convergence of SMD with constant step-size, but only up to a variance ball around the optimum.
Critically, the size of this variance ball does not depend on the step-size, and there is no way to prove the algorithm really converges to the optimum. 

\paragraph{(R6)} 
Thank you for your very encouraging comment.

\bibliographystyle{apalike}
\bibliography{../references}
